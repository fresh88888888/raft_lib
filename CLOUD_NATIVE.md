
#### 蓝绿部署
蓝绿部署是一种以最少的停机时间更新正在运行的计算机系统策略。操作员维护两个环境，称为蓝色和绿色。一个服务于生产流量(所有用户当前使用的版本)，而另一个则进行更新。一旦在非活动(绿色)环境中完成测试，
就会切换生产流量(通常使用负载均衡器)。请注意，蓝绿部署通常意味着一次切换包含许多服务的整个环境。令人困惑的，有时术语用于系统内的个别服务。为避免这种歧义，再提及单个组件时首选术语“零停机部署”。

#### 金丝雀部署
是一种从两个环境开始的部署策略：一个有实时流量；另一个没有实时流量的更新代码。流量逐渐从应用程序的原始版本转移到更新版本。它可以从移动1%的流量开始，然后是10%，25%等等，直到所有版本通过更新版本
运行。组织可以在生产中测试新版本的软件，获得反馈，诊断错误，并在必要时快速回滚到稳定版本。
无论测试策略多么彻底，在生产中总会发现一些错误。将100%的流量从应用程序的一个版本转移到另一个版本可能会导致影响更大的故障。金丝雀(Canary)部署允许组织在将大量流量转移到新版本之前查看新软件在现实场景中的行为。该策略使组织能够最大限度地减少停机时间并在新部署出现问题时快速回滚。它还允许进行更深入的生产应用程序测试，而不会对整体用户体验产生重大影响。

#### 混沌工程
混沌工程或CE是在生产中对分布式系统进行实验的学科，以建立对系统承受动荡和意外情况时能力的信心。
SRE和DevOps实践侧重于提高产品弹性和可靠性的技术。系统在故障容灾时确保服务质量的能力通常是对软件开发提出的要求。这里涉及到几个方面可能导致应用程序中断，例如基础设施、平台（基于微服务）的应用程序
的其他部分。高频地持续部署新功能到生产环境会增加服务中断和恶性事件发生的可能性，乃至于对业务产生重大影响。混沌工程是一种满足弹性要求的技术。它用于实现对基础架构、平台和应用程序等意外发生时的故障
容灾。混沌工程师使用混沌实验主动注入随机故障，已验证应用程序、基础架构或平台是否可以自我修复，并且故障不会对客户产生明显影响。混沌实验旨在发现盲点（例如监控和自动伸缩技术）并在恶性事件发生期间增
强团队之间的沟通。这种方法有助于提高复杂系统的弹性和团队对其的信心。尤其是在生产环境。

#### 零信任架构
零信任架构规定了一种完全消除信任的IT系统设计和实施方法。其核心原则是“永不信任，永远验证”，设备或系统本身在与系统的其他组件进行通信时，总是先验证自检。在今天的许多网络中，在企业网络中，内部的系统和设备可以自由的相互通信，因为他们在企业网络外围的信任边界内。零信任架构采取了相反的方法，虽然在网络边界内，但系统内的组件在进行任何通信之前首先必须通过验证。
在传统的基于信任的方法中，存在于企业网络周边的系统和设备，其假设是，因为有信任，所以没有问题。然而，零信任架构认识到，信任是一个弱点。如果攻击者获得了对受信任设备的访问，根据对该设备的信任和访问
程度，系统现在很容易受到攻击，因为攻击者在受信任的网络边界内，能够在整个系统横向移动。在零信任架构中，信任被移除，因此减少了攻击面，因为攻击者现在被迫在进入整个系统之前进行验证。采用零信任架构带来
的主要好处是增加安全，减少了攻击面。从你的企业系统中移除信任，现在增加了攻击者必须通过的安全门的数量和强度，已获得对系统其他区域的访问。

#### 集群
集群是一组计算机或应用程序，它们为一个共同的目标一起工作。在云原生计算的背景下，这个术语最常被用于Kubernetes，Kubernetes集群是一组服务（或工作负载），它们在各自的容器中运行，通常在不同的机器上
所有这些**容器化**服务的集合，通过网络连接，代表一个集群。
在单台计算机上运行的软件会出现单点故障，如果这台计算机崩溃了，或者有人不小心不掉了电源，那么一些关键的业务系统就会下线。这就是为什么现代软件通常被构建为分布式应用。以集群的形式组合在一起。集群式的
分布式应用在多台机器上运行，消除了单点故障。但构建分布式系统真的很难，事实上，他本身就是一门计算机科学的学科。对全球系统的需求和多年的实验。从而推动了一种新的技术栈的发展：**云原生技术**，这些新
技术使分布式系统的操作和创建变得更容易。

#### 防火墙
防火墙是一个基于特定规则过滤网络流量的系统。防火墙可以是硬件、软件，或者是两者的组合。
默认情况下，在遵循网络的路由规则下，网络将会允许任何人进出，由于这种默认的行为，保护网络安全是有挑战性的。例如，在基于**微服务** 的银行应用程序中，服务之间的沟通是通过其网络来相互传递高敏感性财务
数据。假如没有防火墙，恶意行为者可能会渗透网络、拦截通信并且造成破坏。防火墙使用预设规则来检查网络流量。所有流量都会被过滤，任何来自不可信或可疑来源的流量都会被阻止。只有设置为被接受的流量才能进入
防火墙在安全额受控的内部可信网络间建立了一道屏障。

#### 软件即服务 (SaaS)
软件即服务 (SaaS) 允许用户通过互联网连接或使用云服务。 常见的例子有电邮、日历和办公工具（例如 Gmail、AWS、GitHub、Slack）。 SaaS 以按需付费的方式提供完整的软件解决方案。 所有运维任务和应用数据由服务提供商处理。
传统的商业软件被安装在独立的计算机上，需要管理员维护和更新。 例如：某组织可能在企业内部使用客户关系管理 (CRM) 软件。 该软件需要内部 IT 部门采购、安装、确保安全、维护和定期升级，为 IT 团队增加了负担。 与许可证、安装和潜在附加硬件相关的前期成本可能令人望而却步。 也很难按需响应，很难随着业务增长或变化快速扩缩。SaaS 应用无需内部 IT 部门付出任何特别努力即可工作。 这些应用由供应商安装、维护、升级和确保安全。 扩缩、可用性和容量问题由服务提供商处理，采用按需付费的模式。 对于想要使用企业级应用的各个组织而言，SaaS 是一种经济实惠的方式。

#### 虚拟机
虚拟机（VM）是一台计算机及其操作系统，不受特定硬件的约束。 虚拟机依靠 虚拟化 将一台物理计算机分割成多个虚拟计算机。 这种分离使组织和基础设施供应商能够轻松地创建和销毁虚拟机，而不影响底层硬件。
虚拟机利用了虚拟化的优势。 当 裸机 机器被束缚在一个单一的操作系统上时，该机器的资源的使用受到一定的限制。 另外，当一个操作系统被绑定在一个单一的物理机上时，它的可用性直接与该硬件联系在一起。 如果物理机由于维护或硬件故障而脱机，操作系统也会脱机。
通过消除操作系统和单一物理机之间的直接关系，你解决了裸机的几个问题：配置时间、硬件利用率和弹性。由于不需要购买、安装或配置新的硬件来支持它，新计算机的配置时间得到了极大的改善。 虚拟机通过在一台物理机上放置多个虚拟机，使你能够更好地利用现有的物理硬件资源。 不受特定物理机的约束，虚拟机也比物理机更有弹性。 当一台物理机需要下线时，在其上运行的虚拟机可以被转移到另一台机器上，几乎没有停机时间。

#### 虚拟化
虚拟化，在云原生背景下，是指将一台物理计算机，有时称为服务器，并允许它运行多个隔离的操作系统的过程。这些隔离的操作系统及其专用的计算资源（CPU、内存和网络）被称为虚拟机或VM当我们谈论虚拟机时，
我们在谈论一个软件定义的计算机。它看起来很像一台真正的计算机，但与其他虚拟机共享硬件资源。举个例子，你可以从AWS租赁一台 “计算机”–该计算机实际上是一个虚拟机。
虚拟化解决了许多问题，包括通过允许更多的应用程序在同一台物理机器上运行，同时为了安全起见仍然相互隔离，从而改善物理硬件的使用。在虚拟机上运行的应用程序没有意识到他们正在共享一台物理计算机。 虚拟化还允许数据中心的用户在几分钟内启动一台新的 “计算机”（又称虚拟机），而不必担心在数据中心增加一台新计算机的物理限制。 虚拟机还使用户能够加快获得新的虚拟计算机的时间。

#### 节点
节点是一台能与其他计算机（或节点）协同工作以完成一个共同任务的计算机。以你的笔记本电脑、调制解调器或打印机为例，它们都通过你的WIFI网络进行通信和协作，各自代表一个节点。在云计算中，节点可以是一台物理机，也可以是一台虚拟机（即VM），甚至可以是容器。
虽然一个应用程序可以运行在独立的机器上，但这样做存在一些风险。也就是说，底层系统的故障将会破坏应用程序。为了解决这个问题开发人员开始创建分布式应用程序，其中每个进程都在自己的节点上运行。因此，节点作为集群或节点组的一部分运行着应用程序或进程，而这些节点一起工作已实现共同的目标。节点为你提供了一个可以分配给集群的独特计算单元（CPU、内存和网络）。在云原生平台或应用程序中，一个节点可以代表一个可执行工作的单元。理想情况下，单个节点是不做区分的。因为任何特定类型的节点应该是不可分的。

#### 自动伸缩
自动伸缩，通常是指在计算资源方面，系统能够自动伸缩的能力。自动伸缩系统可在需要时自动添加资源，通过伸缩来满足不断变化的用户需求。自动伸缩的过程各不相同，可基于不同指标进行配置，例如内存和处理时间。托管云服务相较于大多数本地部署环境，有更多的可选项和实施项，因此往往都搭配有自动伸缩的功能。
在此之前，基础设施和应用程序的架构设计会考虑到系统峰值的使用情况这种架构意味着大部分资源没有得到充分利用，并且在面对不断变化的用户需求时缺乏弹性。缺乏弹性则意味着低谷时的业务成本增加，而在高峰时又会由于需求过盛引起的服务中断而导致业务流失。通过利用云、虚拟化、容器化应用程序，组织可以以构建随用户需求而伸缩的应用程序，它们可以监控应用程序的流量并自动伸缩，从而提供最佳的用户体验。以Netflix每周五晚上收视率增长为例，自动伸缩意味着动态添置更多资源：既增加服务器数量以支持以支持更多视频播放需求，并在需求回落后同步缩减。

#### 网站可靠性工程（SRE）
网站可靠性工程（SRE） 是一门结合运营和软件工程的学科，后者特别适合于基础设施和运营问题。这意味着，网站可靠性工程师不是构建产品功能，而是构建系统来运行程序。与DevOps有相似之处，但DevOps专注于将代码投入生产环境，而SRE是确保在生产环境中运行的代码正常工作。
确保应用程序正常工作需要多种功能，从性能监控，警报，调试到故障排除。没有这些，系统操作员只能对问题做出反应，而不是主动去避免它们，停机只是时间问题。网站可靠性工程通过不断改进底层系统来最小化软件开发过程的成本、时间和工作量。 该系统持续测量和监控基础设施和应用程序组件。 当出现问题时，系统会提示网站可靠性工程师何时、何地以及如何修复它。 这种方法通过自动化任务来帮助创建高度可扩展和可靠的软件系统。

#### 紧耦合架构
紧耦合架构（松耦合架构的相反范式）是一种架构风格，其中许多应用程序组件相互依赖，这意味着一个组件的更改可能会影响其他组件。它通常比松耦合架构更容易实现，但会使系统更容易受到级联故障的影响，它意味着需要协调各个组件的部署，这可能会拖累开发人员的生产力。
紧耦合应用程序架构是一种相当传统的应用程序构建方式。 在某些特定情况下，当我们不需要与微服务 开发的所有最佳实践一致时，它将变得很有用。 这意味着更快、更简单地实现， 和单体应用很像 ，可以加快最初的开发周期。

#### 服务网格
在微服务的理念里，应用程序被分解成多个较小的服务，通过网络进行通信。就像你的WIFI网络一样，计算机网络本质上是不可靠的，可被黑客攻击的，而且往往很慢。服务网格通过管理服务之间的流量（即通信），并在所有服务中统一添加可靠性、可观测性和安全功能来解决这一系列新的挑战。
在转向微服务架构之后，工程师们现在要处理数百个，甚至数千个单独的服务，都需要进行通信。这意味着大量的流量在网络上来回传输。除此之外，单个应用程序可能需要通信进行加密，以支持监管要求，为运营团队提供通用指标，或提供对流量的详细洞察，以帮助诊断问题。如果内置于单个应用程序中，这些功能的中的每一个都会引起团队间的冲突，并减缓新功能的开发。服务网格在集群的所有服务中统一增加了可靠性、可观测性和安全功能，而不需要改变代码。在服务网格之前，这些功能必须被编码到各个服务之中，成为错误和技术债务的潜在来源。

#### 服务发现
服务发现是查找组成服务各个实例的过程。 服务发现工具持续跟踪构成服务的各种节点或端点。
云原生架构是动态的和不确定的，这意味着它们不断在变化。 容器化 的应用程序在其生命周期内可能会多次启动和停止。 每次这种情况发生时，它都会有一个新地址，任何应用程序想要找到它，都需要一个工具来提供新的地址信息。服务发现持续跟踪网络中的应用程序，以便在需要时可以找到彼此。 它提供了一个公共的地方来查找和识别不同服务。 服务发现引擎是类似数据库的工具，用于存储当前有哪些服务以及如何找到它们。

#### 服务代理
服务代理拦截进出某项 服务 的流量，对其应用一些逻辑，然后将该流量转发给另一项服务。 它本质上是一个“中间人”，收集有关网络流量的信息，并决定是否对其应用规则。
为了跟踪服务与服务之间的通信（又称网络流量），并可能对其进行转换或重定向，我们需要收集数据。 传统上，实现数据收集和网络流量管理的代码被嵌入每个应用程序中。服务代理允许我们将这种功能“外部化”。它不再需要生活在应用程序中。 相反，它现在被嵌入到平台层中（你的应用程序运行的地方）。作为服务之间的守门员，代理提供对正在发生的通信类型的洞察力。 根据他们的洞察力，他们决定将一个特定的请求发送到哪里，甚至完全拒绝它。代理人收集关键数据，管理路由（在服务之间平均分配流量，或在某些服务中断时重新路由），加密连接，并缓存内容（减少资源消耗）

#### 有状态应用
当我们说到有状态（和无状态）应用时，状态是指应用需要存储以便其按设计运行的的任何数据。 例如，任何能记住您购物车的在线商店都是有状态应用。
使用一个应用通常需要多个请求 例如，使用网上银行时，您将通过输入密码（请求 #1）来验证自己的身份， 然后您可以将钱转给某个朋友（请求 #2），最后您需要查看转账详情（请求 #3）。为了保证正常运行，每一步都必须记住前面的步骤，银行需要记住每个人的账户状态。今天我们使用的大多数应用会存储诸如偏好或设置之类的东西以改善用户体验。有几种方法可以为有状态应用存储状态。最简单的是将状态存储在内存中。这样的问题是每次应用重启时，所有状态都会丢失。为了防止这样的情况发生，状态被持久存储在本地（磁盘上）或数据库系统中。

#### 数据库即服务 (DBaaS)
数据库即服务(DBaaS)是由云厂商管理的服务，它支持应用程序，而无需应用程序团队执行传统的数据库管理功能。DBaaS 允许应用程序开发人员利用数据库，而无需成为专家或聘请数据库管理员来保持数据库最新状态。
传统上，在本地设置中，组织必须定期投资额外的存储和处理能力，以适应可能昂贵的数据库扩展。 此外，开发人员在 IT 基础架构团队的帮助下配置和配置数据库，从而降低了数据库驱动应用程序的部署速度。 加载和执行它们也需要更长的时间。DBaaS 允许开发人员将所有管理/行政操作外包给基于云的服务提供商。 服务提供商确保数据库顺利运行，包括配置管理、备份、补丁、升级、服务监控等，并通过用户友好的界面进行管理。 DBaaS 可帮助组织更快地开发企业级应用程序，同时最大限度地降低数据库成本。

#### 持续集成
持续集成，通常缩写为CI，是尽可能定期集成代码变化的做法，CI是持续交付（CD）的前提，传统上CI过程从代码修改提交到源码控制系统（Git、Mercurial 或 Subversion）时开始，以准备被CD系统使用的测试工件结束。
软件系统通常是庞大而复杂的，有许多开发人员在维护和更新它们。 在系统的不同部分平行工作，这些开发人员可能会做出相互冲突的修改，并在无意中破坏对方的工作。 此外，由于多个开发人员在同一个项目上工作，任何日常工作，如测试和计算代码质量，都需要由每个开发人员重复进行，浪费了时间。每当开发人员提交修改时，CI 软件都会自动检查代码修改是否合并得很干净。 使用 CI 服务器来运行代码质量检查、测试甚至部署，这几乎是一种普遍的做法。 因此，它成为团队内部质量控制的一个具体实施。 CI 允许软件团队把每一个代码提交变成具体的失败或可行的候选发布。

#### 持续部署 (CD)
持续部署，通常缩写为 CD，通过将已经完成的软件直接部署到生产环境，比持续交付更进了一步。持续部署 (CD) 与持续集成(CI) 一起，通常被称为 CI/CD。 CI 流程测试给定应用程序的修改是否正确，CD 流程自动部署企业测试环境的代码更改到生产环境。发布新的软件版本是一个劳动密集且容易出错的过程。这也是企业不想频繁发布新版本的原因，避免生产事故并减少工程师在正常工作时间之外需要随时响应的时间。传统的软件部署模型使组织陷入了一个恶性循环，即发布软件的过程无法同时满足企业在稳定性和软件迭代速度方面的需求。通过自动化发布周期迫使企业更频繁地发布版本到生产环境，CD 为运维团队完成了 CI 为开发团队所做的事情。具体来说，它迫使运维团队将生产部署中痛苦且容易出错的部分自动化，从而降低整体风险。它还使企业能够更好地接受和适应生产环境变化，从而提高稳定性。

#### 持续交付 (CD)
持续交付，通常缩写为 CD，是一套实践，其中代码的变化被自动部署到验收环境中（或者，在持续部署的情况下，部署到生产中）。 CD 关键是包括确保软件在部署前得到充分测试的程序，并提供一种在认为必要时回滚修改的方法。 持续集成（CI）是实现持续交付的第一步（也就是说，在测试和部署之前，变化必须干净地合并）。
部署 可靠 的更新在规模上成为一个问题。 理想情况下，我们会更频繁地部署，为终端用户提供更好的价值。 然而，手动操作会使每一个变化都转化为高额的交易成本。 历史上，为了避免这些成本，企业发布的频率较低，一次部署更多的变化，增加了出错的风险。CD 策略创建了一个完全自动化的生产路径，使用各种部署策略测试和部署软件，如 金丝雀部署 或 蓝绿部署 发布。 这使得开发人员可以频繁地部署代码，让他们放心地认为新的修订版已经过测试。 通常情况下，CD 策略中使用基于主干的开发，而不是功能分支或拉动请求。

#### 微服务
微服务是一种利用云原生技术进行应用开发的现代方法。虽然现代应用程序，如Netflix，看起来是一个单一的应用程序，但它们实际上是一个较小的服务集合，所有的服务都密切配合。简而言之，微服务指的是一种应用架构模式，通常与单体应用形成对比。
微服务是对单体应用所带来的挑战的一种回应。一般来说，一个应用程序的不同部分需要分别进行伸缩。 例如，一个在线商店将有更多的产品视图而不是结账。这意味着你需要更多的产品视图功能的运行，而不是结账。 在一个单一的应用程序中，这些逻辑位不能被单独部署。如果你不能单独扩展产品功能，你将不得不复制整个应用程序和所有其他你不需要的组件–这是一种低效的资源利用。 单机式应用程序也使开发人员容易屈服于设计陷阱。因为所有的代码都在一个地方，所以更容易使这些代码 高耦合，更难执行关注点分离的原则。 单机通常要求开发人员了解整个代码库，然后才能有成效。将功能分离成不同的微服务，使他们更容易分离部署、更新和扩展。通过允许不同的团队专注于更大的应用中它们自己的一小部分，也让他们更容易在不对组织其他部分产生负面影响的情况下对他们的应用进行工作。虽然微服务解决了解决了许多问题，但他们也产生了运营开销，你需要部署和跟踪的东西增加了一个数量级或更多。许多云原生技术旨在使微服务更容易部署和管理。

#### 开发运维 (DevOps)
开发运维是一种方法论，其中团队拥有从应用程序开发到生产操作的整个过程，因此DevOps，它不仅是实施一套技术，还需要文化和流程彻底的转变。DevOps需要一组工程师来处理小组件（相对于整个功能），从而减少交接。
传统上，在具有紧密耦合 单体应用程序的复杂组织中，工作通常分散在多个组之间。这导致了多次交接和较长的交货时间，每次组件更新准备就绪时，都会将其放入队列中以供下一个团队使用。因为个人只参与了项目的一小部分，这种方法导致缺乏所有权。它们的目标是是将工作交给下一个组，而不是为客户提供正确的功能，明显的优先级错位。到代码最终投入生产时，他经过了这么多开发人员，排了这么多队列，如果代码不起作用，很难追查问题的根源。DevOps 颠覆了这种方法。让一个团队拥有应用程序的整个生命周期可以最大限度地减少交接，降低部署到生产中的风险，提高代码质量， 因为团队还负责代码在生产中的执行方式，并且由于更多的自主权和所有权而提高了员工满意度。

#### 平台即服务 (PaaS)
平台即服务（PaaS）是应用程序开发团队部署和运行其应用程序的外部平台。 Heroku、Cloud Foundry、App Engine 是 PaaS 产品的示例。
要利用好 微服务 或 分布式应用程序 等云原生模式， 运维团队和开发人员需要能够免去大量运维工作， 其中包括供应基础设施、处理服务发现 和 负载平衡以及扩展 应用程序等任务。平台即服务（PaaS）以完全自动化的方式为应用程序开发人员提供通用基础设施工具。 它使开发人员可以了解基础设施并减少对基础设施的担忧，并将更多的时间和精力用于编写应用程序代码。 它还提供了一些监控和 可观察性 来帮助应用程序团队确保他们的应用程序是健康的。

#### 容器编排
容器编排指的是在动态的环境中自动管理容器化应用的生命周期。这通过一个容器编排器（大多是Kubernetes）来执行，实现部署、（自动）扩缩、自愈和监控。编排是一个比喻用词：编排工具像一个乐队指挥一样指挥众多容器，确保每个容器各行其是。
手动管理大规模的微服务、安全性和网络通信亦会非常困难。而容器编排能让用户自动化处理所有这些管理任务。容器编排工具允许用户确定系统的状态。首先，这些工具会声明系统应具备的框架（例如x个容器，y个pod）。然后，编排工具将自动监控基础设施，并在其状态偏离声明的状态时对其进行修正（如果一个容器崩溃，则启动一个新的容器）。这种自动化作业精简了许多工程团队原本需要大量手动完成的复杂运营任务，例如制备、部署、扩缩容、联网、负载均衡和其他活动。

#### 容器即服务 (CaaS)
容器即服务 (CaaS)是一种云服务，有助于使用基于容器的抽象管理和部署应用程序，这项服务可以部署在企业内部或云中。CaaS供应商提供了一个框架或协调平台，使容器部署和管理的关键IT功能自动化。它帮助开发者建立安全和可伸缩的容器化应用。因为用户只需要购买他们需要的资源（调度能力，负载均衡等），它们可以节省资金并提高效率。容器创造了一致的环境，以快速开发和交付可以在任何地方运行的云原生应用。
如果没有CaaS,软件开发团需要部署、管理和监控容器运行的底层基础设施。当把容器化应用部署到CaaS平台时，用户可以通过日志聚合和监控工具获得对系统性能额可见性。CaaS还包括自动伸缩和协调管理的内置功能。它使团队能够建立高可见性和高可用性的分布式系统。此外通过允许快速部署，CaaS提高了团队的开发速度在容器确保一致的部署目标额同时，CaaS通过减少管理部署所需的DevOps 资源，降低工程运营成本。

#### 容器化
容器化是将一个应用程序及其依赖关系捆绑到容器镜像中的过程。容器构建过程需要遵守开放容器倡议。
在容器盛行之前，企业依靠虚拟机在一台裸机上协调多个应用程序。虚拟机比容器大得多，需要一个管理程序来运行。由于这些较大的虚拟机模版的存储、备份和传输，创建虚拟机模版也很慢。此外，虚拟机可能会出现配置漂移，这违反了不变性原则。容器镜像是轻量级的，容器化过程需要一个带有依赖性列表的文件。这个文件可以被版本控制，构建过程也可以自动化，允许一个组织在自动化过程中关注其他优先事项。容器镜像由一个唯一的标识符来存储，该标识符与它确切内容和配置相联系。当容器被安排，它们总是被重置为其初始状态，从而消除了配置漂移。

#### 容器
容器是由计算机操作系统管理的具有资源和能力限制的运行过程，容器进程可用的文件被打包为容器镜像。容器在同一台机器上彼此相邻运行，但通常操作系统会阻止单独的容器进程相互交互。
在容器可用之前，需要单独的机器来运行应用程序。每台机器都需要自己的操作系统，它占用 CPU、内存和磁盘空间， 所有这些都是为了让单个应用程序运行。此外，操作系统的维护、升级和启动是另一个重要的工作来源。
容器共享相同的操作系统及其机器资源，分散了操作系统的资源开销并创建了物理机器的有效使用。这种能力之所以成为可能，是因为容器之间的交互通常受到限制。这允许更多的应用程序在同一台物理机器上运行，但是，有一些限制，由于容器共享相同的操作系统。容器还需要对共享资源进行限制。为了保证资源，管理员必须约束和限制内存和CPU的使用，以使其他应用程序不会表现不佳。

#### 安全混沌工程
安全混沌工程（SCE） 是基于混沌工程的学科。 SCE 是指在分布式系统上执行主动安全实验，以建立对系统抵御动荡和恶意条件能力的信心。安全混沌工程师使用科学方法来不断实现这一点，这些方法包括稳态、假设、持续验证、经验教训和缓解实施。
网站稳定性工程师（SRE） 和网络安全工程师的主要职责是尽快恢复服务，来达到零停机时间和最小化业务影响的目标。 SRE 和网络安全工程师共同处理故障前和故障后的各种意外情况。大多数安全问题很难快速发现和修补，这将影响应用程序或系统的功能。此外，在开发阶段通常很难发现安全事件。安全混沌工程是围绕可观察性和网络弹性实践构建的。它旨在发现“不知道不知道”问题并建立对系统的信心，提高网络弹性和可观察性。
工程团队将逐步提高对复杂基础设施、平台和分布式系统中安全问题的理解。 SCE 提高了整个产品的网络弹性，发现隐藏的安全问题，暴露经典盲点，并让团队为关键的边缘案例做好准备。这种方法有助于 SRE、 开发运维和DevSecOps工程师建立对系统的信心，提高网络弹性并提高可观察性。

#### 基础设施即服务 (IaaS)
基础设施即服务，或者 IaaS ，是一种 云计算 服务模型， 它提供 物理 或 虚拟 的计算、存储和网络资源，使用按需按量的计费模式。 云提供商拥有和管理软件和硬件设施，可供消费者在公共、私有或混合云部署和使用。
在搭建传统的本地设施时，组织常常受困于如何保证资源的有效利用。 数据中心建立时必须考虑潜在的高峰需求，即使这样的需求只占1%的使用时间。 而在低需求期间，这些计算资源是空闲的。 而且，如果工作负载超过预期需求，处理工作负载的计算资源则会出现短缺。 这种缺乏可伸缩性的使用方式，将导致成本增加和资源使用率降低。通过 IaaS ，组织可以避免为其应用程序购买和维护计算和数据中心资源。 按需使用的基础设施允许他们根据需要租用计算资源，并推迟大型资本支出， 或 CAPEX ，同时给予他们扩大或缩小规模的灵活性。IaaS 降低了试验或尝试新应用程序的初期成本，并提供了快速部署基础设施的工具。 云提供商是开发或测试环境的绝佳选择，它可以帮助开发人员低成本的进行试验和创新。

#### 基础设施即代码 (IaC)
基础设施即代码是将基础设施的定义存储为一个或多个文件的做法。 这取代了通常通过 shell 脚本或其他配置工具手动配置基础架构即服务的传统模型。
云原生方式构建应用程序要求基础设施是一次性的和可复现的。 它还需要以自动化和可重复的方式按需 伸缩，甚至可能无需人工干预。 手动配置则无法满足 云原生应用 对响应能力和灵活伸缩的诉求。 而且手动基础架构变更不可复现，很容易碰到伸缩瓶颈，并引入错误配置。通过将服务器、负载均衡器和子网等数据中心资源表示为代码， 它允许基础架构团队对所有配置拥有单一的数据源，并允许他们在 CI/CD 通道中管理数据中心，实现版本控制和部署策略。

#### 可观测性
可观测性指的是从所观测的系统采集信号，持续生成并发现可执行的洞察力。换言之，可观测性允许用户从某个系统的外部输出中洞察该系统的状态并采取措施。
计算机系统的衡量机制为观测CPU时间、内存、磁盘空间等底层信号以及每秒API响应次数、每秒错误率、每秒处理的事务数等高级信号和业务信号。系统的可观测性对其运营成本和开发成本有重大影响。可观测系统为操作人员提供了有意义的、可执行的数据，使它们能够达成有利的结果（即更快的事件响应，更高的开发效率）以及更少的艰辛时刻和更短的停机时间。请注意，更多的信息并不一定能转化成为可观测性更好的系统。事实上，有时系统生成的大量信息会形成信息杂音，会使得鉴别有价值的信号变得更加困难。可观测性需要在合适的时间为合适的消费者提供合适的数据，从而做出合适的决策。

#### 可移植性
可移植性是一种软件特征，一种可重用性的形式，有助于避免“锁定”到某些操作环境，例如云提供商、操作系统或供应商。

#### 可伸缩性
可伸缩性指的是一个系统能有多大的发展。这就是增加做任何系统应该做事情的能力。 例如，Kubernetes 集群 通过增加或减少 容器化 应用程序的数量来进行伸缩，但这种可伸缩性取决于几个因素。 它有多少节点，每个节点可以处理多少个容器，控制平面可以支持多少条记录和操作？可伸缩的系统使添加更多容量更容易。主要有两种缩放方法。 一方面，有 水平伸缩 添加更多节点来处理增加的负载。 相比之下，在 垂直伸缩 中，单个节点的功能更强大，可以执行更多事务（例如，通过向单个机器添加更多内存或 CPU）。 可伸缩的系统能够轻松更改并满足用户需求。

#### 双向传输层安全性协议（mTLS）
双向 TLS (mTLS) 是一种用于对两个服务之间发送的消息进行身份验证和加密的技术。 双向 TLS (mTLS) 是标准的传输层安全性协议(TLS) ， 但不是仅验证一个连接的身份，而是验证双方。
微服务通过网络进行通信， 就像您的 wifi 网络一样，通过该网络传输的通信可能会被黑客入侵。 mTLS 确保没有未经授权的一方监听或冒充合法请求。mTLS 确保客户端和服务器之间的双向流量是安全和可信的， 为进入网络或应用程序的用户提供了额外的安全层。 它还验证不遵循登录过程的客户端设备连接，例如物联网 (IoT) 设备。 mTLS 可以防止诸如路径上的攻击、欺骗攻击、凭证填充、暴力攻击等攻击。

#### 分布式系统
分布式系统是通过网络连接的自主计算元素的集合，在用户看来是一个单一额连贯系统。一般被称为节点，这些组件可以是硬件设备或软件进程。节点被编程以实现一个共同的目标，为了协作，他们通过网络交换信息。
今天的许多现代应用程序都非常大，需要超级计算机来操作。想想Gmail或Netflix。 没有一台计算机强大到足以承载整个应用程序。通过连接多台计算机，计算能力几乎变得无限大。 如果没有分布式计算，我们今天依赖的许多应用就不可能实现。传统上，系统会纵向 伸缩。这就是当你在一台单独的机器上添加更多的 CPU 或内存。 垂直伸缩很耗时，需要停机，而且很快就会达到极限。分布式系统允许 水平伸缩（例如，在需要时向系统添加更多节点）。这可以是自动化的，允许系统处理工作负荷或资源消耗的突然增加。非分布式系统面临着故障的风险，因为如果一台机器发生故障，整个系统都会故障。分布式系统可以设计成这样，即使一些机器发生故障，整个系统仍然可以继续工作，产生相同的结果。

#### 分布式应用
分布式应用是一种应用，其功能被分解成多个较小的独立部分。 分布式应用通常由单独的 微服务 组成，处理更广泛应用中的不同问题。 在云原生环境中，各个组件通常作为 容器 在 集群 上运行。
运行在单一计算机上的应用程序代表了一个单点故障–如果该计算机发生故障，应用程序就不可用。 分布式应用通常与单体式应用形成对比。一个单体应用可能更难扩展，因为各种组件不能独立扩展。 随着应用程序的增长，它们也会拖累开发人员的速度，因为更多的开发人员需要在一个不一定有明确边界的共享代码库上工作。当把一个应用程序拆分成不同的部分并在许多地方运行时，整个系统可以容忍更多的故障。 它还允许应用程序利用单个应用程序实例所不具备的伸缩功能，即 水平伸缩的能力。 然而，这也是有代价的：增加了复杂性和操作开销–你现在正在运行很多应用组件，而不是一个应用。

#### 传输层安全性协议（TLS)
传输层安全性协议 (TLS) 是一种旨在为网络通信提供更高安全性的协议。它确保通过因特网发送的数据安全交付，避免可能的数据监视和/或篡改。该协议广泛用于消息传递、电子邮件等应用程序中。
如果没有 TLS，网页浏览习惯、电子邮件通信、在线聊天和电话会议等敏感信息在传输过程中很容易被他人追踪和篡改。启用服务器和客户端应用程序对 TLS 的支持，可以确保它们之间传输的数据是加密的且第三方无法查看。TLS 使用多种编码技术，在通过网络传输数据时提供安全性。 TLS 允许客户端应用程序和服务器（如浏览器和银行站点）之间的加密连接。它还允许客户端应用程序积极地识别他们正在调用的服务器，从而降低客户端与欺诈站点通信的风险。这可以确保第三方无法查看和监控使用 TLS 在应用程序之间传输的数据，从而保护敏感隐私的信息，例如信用卡号、密码、位置等。

#### 云计算
云计算是一种通过互联网按需提供计算资源（CPU、网络和磁盘功能）的模型云计算使用户能在远程物理位置访问和使用计算能力。
传统上，组织在尝试扩展计算能力的使用时面临两个主要问题。他们要么获取、支持、设计和支付托管物理服务器和网络的设施，要么扩展和维护这些设施。云计算允许组织将部分计算需求外包给另一个组织。云提供商为组织提供按需租用计算资源并按使用付费的能力。这允许进行两项主要创新： 组织可以在不浪费时间计划和花费金钱或资源在新的物理基础设施上的情况下进行尝试，并且他们可以根据需要和按需伸缩。 云计算允许组织根据需要采用尽可能多或尽可能少的基础设施。

#### 云原生技术
云原生技术，也称为云原生技术栈，是用于构建云原生应用程序的技术。是组织能在公有云、私有云和混合云等现代动态环境中构建和运行可伸缩的应用程序，利用云计算和容器的功能，微服务、服务网格和不可变的基础设施。

#### 事件驱动架构
事件驱动架构是一种提倡事件的创建、处理和消费的软件架构。事件是对应用程序状态的任何更改。例如，在拼车应用上叫车代表一个事件。这种架构创建了一种结构，在该结构中，事件可以从它们的源，正确地路由到所需的路由器。
随着越来越多的数据变得实时，寻找可靠的方法来确保捕获事件并将其路由到必须处理事件请求的适当服务变得越来越具有挑战性。处理事件的传统方法通常无法保证消息被恰当地路由或发送或接收，随着应用程序的扩展，编排事件变得更具有挑战性。事件驱动架构为所有事件建立了一个中心枢纽(例如，Kafka)。然后定义事件生产者（源）和消费者（接收者），中心事件枢纽保证事件的流动。这种架构确保服务间解耦，并且事件从生产者正确路由到消费者。生产者通常通过HTTP协议传入事件，然后路由事件信息，然后路由事件信息。

#### Serverless
Serverless 是一种云原生开发模型，允许开发人员构建和运行应用程序，而无需管理服务器。 Serverless 中仍有服务器，但它们被 抽象 出来，远离应用程序开发。 云提供商处理配置、维护和 伸缩 服务器基础架构的日常工作。 开发人员可以简单地将他们的代码打包在 容器 中进行部署。 部署后，Serverless 应用程序会响应需求并根据需要自动扩展和缩减。 公共云提供商的 Serverless 产品通常通过事件驱动的执行模型按需计量。 因此，当无服务器功能处于空闲状态时，它不会花费任何费用。
在标准的 基础设施即服务 (IaaS) 云计算 模型下，用户预先购买容量单位， 这意味着您需要向公共云提供商支付永远在线的服务器组件的费用来运行您的应用程序。 用户有责任在高需求时伸缩服务器容量，并在不在需要该容量时缩减容量。 即使在不使用应用程序时，运行应用程序所需的云基础设施也处于活动状态。相比之下，使用 Serverless 架构，应用程序仅在需要时启动。 当事件触发应用程序代码运行时，公共云提供商会为该代码动态分配资源。 当代码执行完成后，用户就停止为资源付款。除了成本和效率优势之外，Serverless 还使开发人员从与应用程序扩展和服务器配置相关的日常和琐碎任务中解放出来。 借助 Serverless，管理操作系统和文件系统、安全补丁、负载平衡、容量管理、伸缩、日志记录和监控等日常任务都被交给云服务提供商。

#### Kubernetes
Kubernetes，通常缩写为K8s，是一种流行的现代基础设施自动化的开源工具。 它就像一个数据中心的操作系统，管理在 分布式系统 上运行的应用程序（就像你笔记本上的操作系统，管理你的应用程序）。
Kubernetes在 集群 的 节点 上调度 容器。 它捆绑了几个基础设施结构，有时被称为 “基元”，如应用程序的实例、负载平衡器、持久性存储等，以一种可以被组成应用程序的方式。
Kubernetes 实现了自动化和可扩展性，使用户能够以可重复的方式声明性地部署应用程序。 Kubernetes 生态系统中的软件产品和项目利用这种自动化和可扩展性来扩展 Kubernetes API 。 这使他们能够利用 Kubernetes 的自动化，并使他们的工具更容易被有经验的 Kubernetes 从业者所接受。
长期以来，基础设施自动化和声明性配置管理一直是重要的概念，而且随着 云计算 的普及而变得更加紧迫。 随着对计算资源的需求增加，组织感到压力，要用更少的工程师提供更多的操作能力，需要新的技术和工作方法来满足这一需求。 此外，云计算的兴起与 容器化 相搭配，那些忙于自动化更多传统基础设施的组织需要一种机制来自动配置和部署其容器。Kubernetes 以类似于传统的基础设施即代码工具的方式帮助实现自动化，但它的优势在于，与虚拟机或物理机相比，容器更能抵抗配置漂移。 Kubernetes 的工作方式是声明式的，这意味着操作者不是提供关于如何做某事的指示，而是描述（通常是YAML文档）他们想要做什么；Kubernetes将自行处理 “如何 “的问题。这导致Kubernetes与基础设施即代码极为兼容。Kubernetes 还能自我修复。这意味着它确保集群的实际状态总是与操作者的期望状态相匹配。 如果Kubernetes检测到一个偏差，Kubernetes 控制器就会启动并修复它。 因此，虽然它使用的基础设施可能不断变化，但 Kubernetes 本身也在不断自动适应变化，并确保它与预期状态相匹配。

#### API网关
API网关是一种通过聚合多个应用程序的API，并实现一站式管理的工具，它允许组织将关键性功能移交到一个可集中管理的地方，例如身份验证和授权、限制应用程序之间的请求数量。一个API网关则作为一个公共接口，向API消费者（通常来自外部）提供服务。
当你额应用需要向外部消费者提供API时，你通常需要一个统一的入口来管理和控制所有的访问。此外，如果你需要对这些交互添加某种功能，也可以在不更改任何应用代码的情况下为所有的流量实现新功能。通过为多个多个API提供一个统一的访问入口，API网关能够让组织更容易地将交叉业务或安全性逻辑移交到移交到一个可集中管理的地方，应用到消费端也只需要访问单个地址就可以满足其所有需求。通过系统中的所有Web提供统一的访问入口，API网关还可以简化诸如安全性和可观测性之类的运维问题。由于所有请求都流经API网关，因此他可以中心化的为这些请求添加诸如指标收集、速率限制和授权等功能。


#### 分布式系统
- 可扩展性（Scalability）
- 可用性（Availability）
- 一致性（Consistency）

#### 分布式系统难点
构建分布式系统大多都是关于如何设计存储系统，或是设计其它基于大型分布式存储系统。所以我们会更加关注如何为大型分布式系统设计一个优秀的接口，以及如何设计存储系统的内部结构，这样系统才能良好运行。
##### GFS Master
GRS master 节点内保存的数据内容，这里边我们关心的主要是两个表单。
- 第一个是文件名到Chunk ID或者Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk。但是只有Chunk ID是做不了太多事情的，所以有了第二个表单。
- 第二个表单记录了Chunk ID到Chunk数据的对应关系。这里的数据又包括了：每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表；每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号；所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk；并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间。

如果Master故障了，这些数据就都丢失了。为了能让Master重启而不丢失数据，Master节点会同时将数据存储在磁盘上。所以Master节点读数据会从内存中都，但写数据的时候，至少有一部分数据会接入到磁盘当中。更具体来说，Master会在磁盘上存储Log，每次有数据变更时，Master会在磁盘的Log种增加一条记录，并生成CheckPoint(类似于备份点)。有些数据需要存在磁盘上，有些数据则不用。
- Chunk Handle的数组（第一个表单）要保存在磁盘上。我给它标记成NV（non-volatile, 非易失），这个标记表示对应的数据会写入到磁盘上。
- Chunk服务器列表不用保存到磁盘上。因为Master节点重启之后可以与所有的Chunk服务器通信，并查询每个Chunk服务器存储了哪些Chunk，所以我认为它不用写入磁盘。所以这里标记成V（volatile）
- 版本号要不要写入磁盘取决于GFS是如何工作的，我认为它需要写入磁盘。我们之后在讨论系统是如何工作的时候再详细讨论这个问题。这里先标记成NV。
- 主Chunk的ID，几乎可以确定不用写入磁盘，因为Master节点重启之后会忘记谁是主Chunk，它只需要等待60秒租约到期，那么它知道对于这个Chunk来说没有主Chunk，这个时候，Master节点可以安全指定一个新的主Chunk。所以这里标记成V。
- 类似的，租约过期时间也不用写入磁盘，所以这里标记成V。

任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于制定了新的主Chunk而导致版本号更新了，Master节点需要向中的Log增加一条记录说我刚刚向这个文件添加了一个新的Chunk或者我刚刚修改了Chunk的版本号。所以每次有这样的更新，都需要写磁盘。由于写磁盘的速度是有限的，写磁盘会导致Master节点的更新速度也是有限的，所以要尽可能少的写入数据到磁盘。
当Master节点故障重启，并重建它的状态，你不会想要从Log的最开始重建状态，因为Log的最开始可能是几年前，所以Master节点会在磁盘中创建一些checkpoint点，这可能要花费几秒甚至一分钟。这样在Master重启时，会从Log最近一个的checkpoint开始恢复，再逐条执行从checkpoint开始的Log，最后恢复自己的状态。

##### GFS读文件
对于读请求来说，意味着应用程序或者GFS客户端有一个文件名和它想从文件的某个位置读取的偏移量（offset），应用程序会将这些信息发送给Master节点，Master节点会从自己的file表单中查询文件名，得到Chunk ID的数组，因为每个Chunk是64MB，所以偏移量除以64MB就可从数组中得到对应的Chunk ID。之后Master再从Chunk表单中找到存有Chunk服务器列表，并将列表返回给客户端。所以，第一步是客户端（或应用程序）将文件名和偏移量发送给Master；第二步，Master节点通过偏移量计算得到Chunk ID数组，然后在查找出Chunk服务器列表，并发送给客户端。

##### GFS写文件
当有多个客户端同时写同一个文件时，一个客户端并不能知道文件究竟有多长。因为如果只有一个客户端在写文件，客户端自己可以记录文件长度，而多个客户端时，一个客户端没法知道其他客户端写了多少。例如，不同客户端写同一份日志文件，没有一个客户端会知道文件究竟有多长，因此也就不知道该往什么样的偏移量，或者说向哪个Chunk去追加数据。这个时候，客户端可以向Master节点查询哪个Chunk服务器保存了文件的最后一个Chunk。
对于读文件来说，可以从任何最新的Chunk副本读取数据，但是对于写文件来说，必须要通过Chunk的主副本（Primary Chunk）来写入。对于某个特定的Chunk来说，在某一个时间点，Master不一定指定了Chunk的主副本。所以，写文件的时候，需要考虑Chunk的主副本不存在的情况。
对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。如果你的一个系统已经运行了很长时间，那么有可能某一个Chunk服务器保存的Chunk副本是旧的，比如说还是昨天或者上周的。导致这个现象的原因可能是服务器因为宕机而没有收到任何的更新。所以，Master节点需要能够在Chunk的多个副本中识别出，哪些副本是新的，哪些是旧的。所以第一步是，找出新的Chunk副本。这一切都是在Master节点发生，因为，现在是客户端告诉Master节点说要追加某个文件，Master节点需要告诉客户端向哪个Chunk服务器（也就是Primary Chunk所在的服务器）去做追加操作。所以，Master节点的部分工作就是弄清楚在追加文件时，客户端应该与哪个Chunk服务器通信。
每个Chunk可能同时有多个副本，最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致。Chunk副本中的版本号是由Master节点下发的，所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的。这就是为什么Chunk的版本号在Master节点上需要保存在磁盘这种非易失的存储中的原因（见3.4），因为如果版本号在故障重启中丢失，且部分Chunk服务器持有旧的Chunk副本，这时，Master是没有办法区分哪个Chunk服务器的数据是旧的，哪个Chunk服务器的数据是最新的。
回到之前的话题，当客户端想要对文件进行追加，但是又不知道文件尾的Chunk对应的Primary在哪时，Master会等所有存储了最新Chunk版本的服务器集合完成，然后挑选一个作为Primary，其他的作为Secondary。之后，Master会增加版本号，并将版本号写入磁盘，这样就算故障了也不会丢失这个数据。
接下来，Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们，谁是Primary，谁是Secondary，Chunk的新版本是什么。Primary和Secondary服务器都会将版本号存储在本地的磁盘中。这样，当它们因为电源故障或者其他原因重启时，它们可以向Master报告本地保存的Chunk的实际版本号。

##### GFS的一致性
当我们追加数据时，面对Chunk的三个副本，当客户端发送了一个追加数据的请求，要将数据A追加到文件末尾，所有的三个副本，包括一个Primary和两个Secondary，都成功的将数据追加到了Chunk，所以Chunk中的第一个记录是A。假设第二个客户端加入进来，想要追加数据B，但是由于网络问题发送给某个副本的消息丢失了。所以，追加数据B的消息只被两个副本收到，一个是Primary，一个是Secondary。这两个副本都在文件中追加了数据B，所以，现在我们有两个副本有数据B，另一个没有。之后，第三个客户端想要追加数据C，并且第三个客户端记得下图中左边第一个副本是Primary。Primary选择了偏移量，并将偏移量告诉Secondary，将数据C写在Chunk的这个位置。三个副本都将数据C写在这个位置。对于数据B来说，客户端会收到写入失败的回复，客户端会重发写入数据B的请求。所以，第二个客户端会再次请求追加数据B，或许这次数据没有在网络中丢包，并且所有的三个副本都成功追加了数据B。现在三个副本都在线，并且都有最新的版本号。之后，如果一个客户端读文件，读到的内容取决于读取的是Chunk的哪个副本。客户端总共可以看到三条数据，但是取决于不同的副本，读取数据的顺序是不一样的。如果读取的是第一个副本，那么客户端可以读到A、B、C，然后是一个重复的B。如果读取的是第三个副本，那么客户端可以读到A，一个空白数据，然后是C、B。所以，如果读取前两个副本，B和C的顺序是先B后C，如果读的是第三个副本，B和C的顺序是先C后B。所以，不同的读请求可能得到不同的结果。
或许最坏的情况是，一些客户端写文件时，因为其中一个Secondary未能成功执行数据追加操作，客户端从Primary收到写入失败的回复。在客户端重新发送写文件请求之前，客户端就故障了。所以，你有可能进入这种情形：数据D出现在某些副本中，而其他副本则完全没有。在GFS的这种工作方式下，如果Primary返回写入成功，那么一切都还好，如果Primary返回写入失败，就不是那么好了。Primary返回写入失败会导致不同的副本有完全不同的数据。

##### Leader选举（Leader Election）
那Leader是如何创建出来的呢？每个Raft节点都有一个选举定时器（Election Timer），如果在这个定时器时间耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举。所以我们这里有了这个选举定时器，当它的时间耗尽时，当前节点会开始一次选举。开始一次选举的意思是，当前服务器会增加任期号（term number），因为它想成为一个新的Leader。而你知道的，一个任期内不能有超过一个Leader，所以为了成为一个新的Leader，这里需要开启一个新的任期。 之后，当前服务器会发出请求投票（RequestVote）RPC，这个消息会发给所有的Raft节点。其实只需要发送到N-1个节点，因为Raft规定了，Leader的候选人总是会在选举时投票给自己。这里需要注意的一点是，并不是说如果Leader没有故障，就不会有选举。但是如果Leader的确出现了故障，那么一定会有新的选举。这个选举的前提是其他服务器还在运行，因为选举需要其他服务器的选举定时器超时了才会触发。另一方面，如果Leader没有故障，我们仍然有可能会有一次新的选举。比如，如果网络很慢，丢了几个心跳，或者其他原因，这时，尽管Leader还在健康运行，我们可能会有某个选举定时器超时了，进而开启一次新的选举。在考虑正确性的时候，我们需要记住这点。所以这意味着，如果有一场新的选举，有可能之前的Leader仍然在运行，并认为自己还是Leader。例如，当出现网络分区时，旧Leader始终在一个小的分区中运行，而较大的分区会进行新的选举，最终成功选出一个新的Leader。为了能够当选，Raft要求一个候选人从过半服务器中获得认可投票。每个Raft节点，只会在一个任期内投出一个认可选票。这意味着，在任意一个任期内，每一个节点只会对一个候选人投一次票。这样，就不可能有两个候选人同时获得过半的选票，因为每个节点只会投票一次。所以这里是过半原则导致了最多只能有一个胜出的候选人，这样我们在每个任期会有最多一个选举出的候选人。同时，也是非常重要的一点，过半原则意味着，即使一些节点已经故障了，你仍然可以赢得选举。如果少数服务器故障了或者出现了网络问题，我们仍然可以选举出Leader。如果超过一半的节点故障了，不可用了，或者在另一个网络分区，那么系统会不断地额尝试选举Leader，并永远也不能选出一个Leader，因为没有过半的服务器在运行。如果一次选举成功了，整个集群的节点是如何知道的呢？当一个服务器赢得了一次选举，这个服务器会收到过半的认可投票，这个服务器会直接知道自己是新的Leader，因为它收到了过半的投票。但是其他的服务器并不能直接知道谁赢得了选举，其他服务器甚至都不知道是否有人赢得了选举。这时，（赢得了选举的）候选人，会通过心跳通知其他服务器。Raft论文的图2规定了，如果你赢得了选举，你需要立刻发送一条AppendEntries消息给其他所有的服务器。这条代表心跳的AppendEntries并不会直接说：我赢得了选举，我就是任期23的Leader。这里的表达会更隐晦一些。Raft规定，除非是当前任期的Leader，没人可以发出AppendEntries消息。所以假设我是一个服务器，我发现对于任期19有一次选举，过了一会我收到了一条AppendEntries消息，这个消息的任期号就是19。那么这条消息告诉我，我不知道的某个节点赢得了任期19的选举。所以，其他服务器通过接收特定任期号的AppendEntries来知道，选举成功了。

##### 选举定时器（Election Timer）
任何一条AppendEntries消息都会重置所有Raft节点的选举定时器。这样，只要Leader还在线，并且它还在以合理的速率（不能太慢）发出心跳或者其他的AppendEntries消息，Followers收到了AppendEntries消息，会重置自己的选举定时器，这样Leader就可以阻止任何其他节点成为一个候选人。所以只要所有环节都在正常工作，不断重复的心跳会阻止任何新的选举发生。当然，如果网络故障或者发生了丢包，不可避免的还是会有新的选举。但是如果一切都正常，我们不太可能会有一次新的选举。如果一次选举选出了0个Leader，这次选举就失败了。有一些显而易见的场景会导致选举失败，例如太多的服务器关机或者不可用了，或者网络连接出现故障。这些场景会导致你不能凑齐过半的服务器，进而也不能赢得选举，这时什么事也不会发生。一个导致选举失败的更有趣的场景是，所有环节都在正常工作，没有故障，没有丢包，但是候选人们几乎是同时参加竞选，它们分割了选票（Split Vote）。假设我们有一个3节点的多副本系统，3个节点的选举定时器几乎同超时，进而期触发选举。首先，每个节点都会为自己投票。之后，每个节点都会收到其他节点的RequestVote消息，因为该节点已经投票给自己了，所以它会返回反对投票。这意味着，3个节点中的每个节点都只能收到一张投票（来自于自己）。没有一个节点获得了过半投票，所以也就没有人能被选上。接下来它们的选举定时器会重新计时，因为选举定时器只会在收到了AppendEntries消息时重置，但是由于没有Leader，所有也就没有AppendEntries消息。所有的选举定时器重新开始计时，如果我们不够幸运的话，所有的定时器又会在同一时间到期，所有节点又会投票给自己，又没有人获得了过半投票，这个状态可能会一直持续下去。
Raft不能完全避免分割选票（Split Vote），但是可以使得这个场景出现的概率大大降低。Raft通过为选举定时器随机的选择超时时间来达到这一点。我们可以这样来看这种随机的方法。假设这里有个时间线，我会在上面画上事件。在某个时间，所有的节点收到了最后一条AppendEntries消息。之后，Leader就故障了。我们这里假设Leader在发出最后一次心跳之后就故障关机了。所有的Followers在同一时间重置了它们的选举定时器，因为它们大概率在同一时间收到了这条AppendEntries消息。

这里对于选举定时器的超时时间的设置，需要注意一些细节。一个明显的要求是，选举定时器的超时时间需要至少大于Leader的心跳间隔。这里非常明显，假设Leader每100毫秒发出一个心跳，你最好确认所有节点的选举定时器的超时时间不要小于100毫秒，否则该节点会在收到正常的心跳之前触发选举。所以，选举定时器的超时时间下限是一个心跳的间隔。实际上由于网络可能丢包，这里你或许希望将下限设置为多个心跳间隔。所以如果心跳间隔是100毫秒，你或许想要将选举定时器的最短超时时间设置为300毫秒，也就是3次心跳的间隔。所以，如果心跳间隔是这么多（两个AE之间），那么你会想要将选举定时器的超时时间下限设置成心跳间隔的几倍。
那超时时间的上限呢？因为随机的话都是在一个范围内随机，那我们应该在哪设置超时时间的上限呢？在一个实际系统中，有几点需要注意。首先，这里的最大超时时间影响了系统能多快从故障中恢复。因为从旧的Leader故障开始，到新的选举开始这段时间，整个系统是瘫痪了。尽管还有一些其他服务器在运行，但是因为没有Leader，客户端请求会被丢弃。所以，这里的上限越大，系统的恢复时间也就越长。这里究竟有多重要，取决于我们需要达到多高的性能，以及故障出现的频率。如果一年才出一次故障，那就无所谓了。如果故障很频繁，那么我们或许就该关心恢复时间有多长。这是一个需要考虑的点。另一个需要考虑的点是，不同节点的选举定时器的超时时间差（S2和S3之间）必须要足够长，使得第一个开始选举的节点能够完成一轮选举。这里至少需要大于发送一条RPC所需要的往返（Round-Trip）时间。或许需要10毫秒来发送一条RPC，并从其他所有服务器获得响应。如果这样的话，我们需要设置超时时间的上限到足够大，从而使得两个随机数之间的时间差极有可能大于10毫秒。

这里还有一个小点需要注意，每一次一个节点重置自己的选举定时器时，都需要重新选择一个随机的超时时间。也就是说，不要在服务器启动的时候选择一个随机的超时时间，然后反复使用同一个值。因为如果你不够幸运的话，两个服务器会以极小的概率选择相同的随机超时时间，那么你会永远处于分割选票的场景中。所以你需要每次都为选举定时器选择一个不同的随机超时时间。

##### 日志恢复（Log Backup）

##### 选举约束（Election Restriction）
为了保证系统的正确性，并非任意节点都可以成为Leader。不是说第一个选举定时器超时了并触发选举的节点，就一定是Leader。Raft对于谁可以成为Leader，谁不能成为Leader是有一些限制的。
为了证明并非任意节点都可以成为Leader，我们这里提出一个例子来证伪。在这个反例中，Raft会选择拥有最长Log记录的节点作为Leader，这个规则或许适用于其他系统，实际上在一些其他设计的系统中的确使用了这样的规则，但是在Raft中，这条规则不适用。所以，我们这里需要研究的问题是：为什么不选择拥有最长Log记录的节点作为Leader？如果我们这么做了的话，我们需要更改Raft中的投票规则，让选民只投票给拥有更长Log记录的节点。很容易可以展示为什么这是一个错误的观点。我们还是假设我们有3个服务器，现在服务器1（S1）有任期5，6，7的Log，服务器2和服务器3（S2和S3）有任期5，8的Log。为了避免我们在不可能出现的问题上浪费时间，这里的第一个问题是，这个场景可能出现吗？让我们回退一些时间，在这个时间点S1赢得了选举，现在它的任期号是6。它收到了一个客户端请求，在发出AppendEntries之前，它先将请求存放在自己的Log中，然后它就故障了，所以它没能发出任何AppendEntries消息。之后它很快就故障重启了，因为它是之前的Leader，所以会有一场新的选举。这次，它又被选为Leader。然后它收到了一个任期7的客户端请求，将这个请求加在本地Log之后，它又故障了。S1故障之后，我们又有了一次新的选举，这时S1已经关机了，不能再参加选举，这次S2被选为Leader。如果S2当选，而S1还在关机状态，S2会使用什么任期号呢？明显我们的答案是8（因为之前画出来了），但是为什么任期号是8而不是6呢？尽管没有写在黑板上，但是S1在任期6，7能当选，它必然拥有了过半节点的投票，过半服务器至少包含了S2，S3中的一个节点。如果你去看处理RequestVote的代码和Raft论文的图2，当某个节点为候选人投票时，节点应该将候选人的任期号记录在持久化存储中。所里在这里，S2或者S3或者它们两者都知道任期6和任期7的存在。因此，当S1故障了，它们中至少一个知道当前的任期是8。这里，只有知道了任期8的节点才有可能当选，如果只有一个节点知道，那么这个节点会赢得选举，因为它拥有更高的任期号。如果S2和S3都知道当前任期是8，那么它们两者中的一个会赢得选举。所以，下一个任期必然为8这个事实，依赖于不同任期的过半服务器之间必然有重合这个特点。同时，也依赖任期号会通过RequestVote RPC更新给其他节点，并持久化存储，这样出现故障才不会丢失数据。所以下一个任期号将会是8，S2或者S3会赢得选举。不管是哪一个，新的Leader会继续将客户端请求转换成AppendEntries发给其他节点。所以我们现在有了这么一个场景。现在我们回到对于这个场景的最初的问题，假设S1重新上线了，并且我们又有了一次新的选举，这时候可以选择S1作为Leader吗？或者说，可以选择拥有最长Log记录的节点作为Leader可以吗？明显，答案是不可以的。
如果S1是Leader，它会通过AppendEntries机制将自己的Log强加给2个Followers，这个我们刚刚（上一节）说过了。如果我们让S1作为Leader，它会发出AppendEntries消息来覆盖S2和S3在任期8的Log，并在S2和S3中写入S1中的任期6和任期7的Log，这样所有的节点的Log才能与S1保持一致。为什么我们不能认可这样的结果呢？
是的，因为S2和S3可以组成过半服务器，所以任期8的Log已经被commit了，对应的请求很可能已经执行了，应用层也很可能发送一个回复给客户端了。所以我们不能删除任期8的Log。因此，S1也就不能成为Leader并将自己的Log强制写入S2和S3。大家都明白了为什么这对于Raft来说是个坏的结果吗？正因为这个原因，我们不能在选举的时候直接选择拥有最长Log记录的节点。当然，最短Log记录的节点也不行。
在Raft论文的5.4.1，Raft有一个稍微复杂的选举限制（Election Restriction）。这个限制要求，在处理别节点发来的RequestVote RPC时，需要做一些检查才能投出赞成票。这里的限制是，节点只能向满足下面条件之一的候选人投出赞成票：
- 候选人最后一条Log条目的任期号大于本地最后一条Log条目的任期号；
- 或者，候选人最后一条Log条目的任期号等于本地最后一条Log条目的任期号，且候选人的Log记录长度大于等于本地Log记录的长度

回到我们的场景，如果S2收到了S1的RequestVote RPC，因为S1的最后一条Log条目的任期号是7，而S2的最后一条Log条目的任期号是8，两个限制都不满足，所以S2和S3都不会给S1投赞成票。即使S1的选举定时器的超时时间更短，并且先发出了RequestVote请求，除了它自己，没人会给它投票，所以它只能拿到一个选票，不能凑够过半选票。如果S2或者S3成为了候选人，它们中的另一个都会投出赞成票，因为它们最后的任期号一样，并且它们的Log长度大于等于彼此（满足限制2）。所以S2或者S3中的任意一个都会为另一个投票。S1会为它们投票吗？会的，因为S2或者S3最后一个Log条目对应的任期号更大（满足限制1）。
所以在这里，Raft更喜欢拥有更高任期号记录的候选人，或者说更喜欢拥有任期号更高的旧Leader记录的候选人。限制2说明，如果候选人都拥有任期号最高的旧Leader记录，那么Raft更喜欢拥有更多记录的候选人。

##### 快速恢复（Fast Backup）
- 场景1。Follower（S1）会返回XTerm=5，XIndex=2。Leader（S2）发现自己没有任期5的日志，它会将自己本地记录的，S1的nextIndex设置到XIndex，也就是S1中，任期5的第一条Log对应的槽位号。所以，如果Leader完全没有XTerm的任何Log，那么它应该回退到XIndex对应的位置（这样，Leader发出的下一条AppendEntries就可以一次覆盖S1中所有XTerm对应的Log）。
- 场景2。Follower（S1）会返回XTerm=4，XIndex=1。Leader（S2）发现自己其实有任期4的日志，它会将自己本地记录的S1的nextIndex设置到本地在XTerm位置的Log条目后面，也就是槽位2。下一次Leader发出下一条AppendEntries时，就可以一次覆盖S1中槽位2和槽位3对应的Log。
- 场景3。Follower（S1）会返回XTerm=-1，XLen=2。这表示S1中日志太短了，以至于在冲突的位置没有Log条目，Leader应该回退到Follower最后一条Log条目的下一条，也就是槽位2，并从这开始发送AppendEntries消息。槽位2可以从XLen中的数值计算得到。

##### 持久化（Persistence）
你或许会认为，如果一个服务器故障了，那简单直接的方法就是将它从集群中摘除。我们需要具备从集群中摘除服务器，替换一个全新的空的服务器，并让该新服务器在集群内工作的能力。实际上，这是至关重要的，因为如果一些服务器遭受了不可恢复的故障，例如磁盘故障，你绝对需要替换这台服务器。同时，如果磁盘故障了，你也不能指望能从该服务器的磁盘中获得任何有用的信息。所以我们的确需要能够用全新的空的服务器替代现有服务器的能力。你或许认为，这就足以应对任何出问题的场景了，但实际上不是的。
实际上，一个常见的故障是断电。断电的时候，整个集群都同时停止运行，这种场景下，我们不能通过从Dell买一些新的服务器来替换现有服务器进而解决问题。这种场景下，如果我们希望我们的服务是容错的， 我们需要能够得到之前状态的拷贝，这样我们才能保持程序继续运行。因此，至少为了处理同时断电的场景，我们不得不让服务器能够将它们的状态存储在某处，这样当供电恢复了之后，还能再次获取这个状态。这里的状态是指，为了让服务器在断电或者整个集群断电后，能够继续运行所必不可少的内容。这是理解持久化存储的一种方式。
在Raft论文的图2中，有且仅有三个数据是需要持久化存储的。它们分别是Log、currentTerm、votedFor。Log是所有的Log条目。当某个服务器刚刚重启，在它加入到Raft集群之前，它必须要检查并确保这些数据有效的存储在它的磁盘上。服务器必须要有某种方式来发现，自己的确有一些持久化存储的状态，而不是一些无意义的数据。
那currentTerm呢？为什么currentTerm需要被持久化存储？是的，currentTerm和votedFor都是用来确保每个任期只有最多一个Leader。在一个故障的场景中，如果一个服务器收到了一个RequestVote请求，并且为服务器1投票了，之后它故障。如果它没有存储它为哪个服务器投过票，当它故障重启之后，收到了来自服务器2的同一个任期的另一个RequestVote请求，那么它还是会投票给服务器2，因为它发现自己的votedFor是空的，因此它认为自己还没投过票。现在这个服务器，在同一个任期内同时为服务器1和服务器2投了票。因为服务器1和服务器2都会为自己投票，它们都会认为自己有过半选票（3票中的2票），那它们都会成为Leader。现在同一个任期里面有了两个Leader。这就是为什么votedFor必须被持久化存储。currentTerm的情况要更微妙一些，但是实际上还是为了实现一个任期内最多只有一个Leader，我们之前实际上介绍过这里的内容。如果（重启之后）我们不知道任期号是什么，很难确保一个任期内只有一个Leader。 
在这里例子中，S1关机了，S2和S3会尝试选举一个新的Leader。它们需要证据证明，正确的任期号是8，而不是6。如果仅仅是S2和S3为彼此投票，它们不知道当前的任期号，它们只能查看自己的Log，它们或许会认为下一个任期是6（因为Log里的上一个任期是5）。如果它们这么做了，那么它们会从任期6开始添加Log。但是接下来，就会有问题了，因为我们有了两个不同的任期6（另一个在S1中）。这就是为什么currentTerm需要被持久化存储的原因，因为它需要用来保存已经被使用过的任期号。
这些数据需要在每次你修改它们的时候存储起来。所以可以确定的是，安全的做法是每次你添加一个Log条目，更新currentTerm或者更新votedFor，你或许都需要持久化存储这些数据。在一个真实的Raft服务器上，这意味着将数据写入磁盘，所以你需要一些文件来记录这些数据。如果你发现，直到服务器与外界通信时，才有可能持久化存储数据，那么你可以通过一些批量操作来提升性能。例如，只在服务器回复一个RPC或者发送一个RPC时，服务器才进行持久化存储，这样可以节省一些持久化存储的操作。
之所以这很重要是因为，向磁盘写数据是一个代价很高的操作。如果是一个机械硬盘，我们通过写文件的方式来持久化存储，向磁盘写入任何数据都需要花费大概10毫秒时间。因为你要么需要等磁盘将你想写入的位置转到磁针下面， 而磁盘大概每10毫秒转一次。要么，就是另一种情况更糟糕，磁盘需要将磁针移到正确的轨道上。所以这里的持久化操作的代价可能会非常非常高。对于一些简单的设计，这些操作可能成为限制性能的因素，因为它们意味着在这些Raft服务器上执行任何操作，都需要10毫秒。而10毫秒相比发送RPC或者其他操作来说都太长了。如果你持久化存储在一个机械硬盘上，那么每个操作至少要10毫秒，这意味着你永远也不可能构建一个每秒能处理超过100个请求的Raft服务。这就是所谓的synchronous disk updates的代价。它存在于很多系统中，例如运行在你的笔记本上的文件系统。
设计人员花费了大量的时间来避开synchronous disk updates带来的性能问题。为了让磁盘的数据保证安全，同时为了能安全更新你的笔记本上的磁盘，文件系统对于写入操作十分小心，有时需要等待磁盘（前一个）写入完成。所以这（优化磁盘写入性能）是一个出现在所有系统中的常见的问题，也必然出现在Raft中。
如果你想构建一个能每秒处理超过100个请求的系统，这里有多个选择。其中一个就是，你可以使用SSD硬盘，或者某种闪存。SSD可以在0.1毫秒完成对于闪存的一次写操作，所以这里性能就提高了100倍。更高级一点的方法是，你可以构建一个电池供电的DRAM，然后在这个电池供电的DRAM中做持久化存储。这样，如果Server重启了，并且重启时间短于电池的可供电时间，这样你存储在RAM中的数据还能保存。如果资金充足，且不怕复杂的话，这种方式的优点是，你可以每秒写DRAM数百万次，那么持久化存储就不再会是一个性能瓶颈。所以，synchronous disk updates是为什么数据要区分持久化和非持久化（而非所有的都做持久化）的原因（越少数据持久化，越高的性能）。Raft论文图2考虑了很多性能，故障恢复，正确性的问题。
所以你可以使用一些更贵的磁盘。另一个常见方法是，批量执行操作。如果有大量的客户端请求，或许你应该同时接收它们，但是先不返回。等大量的请求累积之后，一次性持久化存储（比如）100个Log，之后再发送AppendEntries。如果Leader收到了一个客户端请求，在发送AppendEntries RPC给Followers之前，必须要先持久化存储在本地。因为Leader必须要commit那个请求，并且不能忘记这个请求。实际上，在回复AppendEntries 消息之前，Followers也需要持久化存储这些Log条目到本地，因为它们最终也要commit这个请求，它们不能因为重启而忘记这个请求。
最后，有关持久化存储，还有一些细节。有些数据在Raft论文的图2中标记为非持久化的。所以，这里值得思考一下，为什么服务器重启时，commitIndex、lastApplied、nextIndex、matchIndex，可以被丢弃？例如，lastApplied表示当前服务器执行到哪一步，如果我们丢弃了它的话，我们需要重复执行Log条目两次（重启前执行过一次，重启后又要再执行一次），这是正确的吗？为什么可以安全的丢弃lastApplied？
这里综合考虑了Raft的简单性和安全性。之所以这些数据是非持久化存储的，是因为Leader可以通过检查自己的Log和发送给Followers的AppendEntries的结果，来发现哪些内容已经commit了。如果因为断电，所有节点都重启了。Leader并不知道哪些内容被commit了，哪些内容被执行了。但是当它发出AppendEntries，并从Followers搜集回信息。它会发现，Followers中有哪些Log与Leader的Log匹配，因此也就可以发现，在重启前，有哪些被commit了。
另外，Raft论文的图2假设，应用程序状态会随着重启而消失。所以图2认为，既然Log已经持久化存储了，那么应用程序状态就不必再持久化存储。因为在图2中，Log从系统运行的初始就被持久化存储下来。所以，当Leader重启时，Leader会从第一条Log开始，执行每一条Log条目，并提交给应用程序。所以，重启之后，应用程序可以通过重复执行每一条Log来完全从头构建自己的状态。这是一种简单且优雅的方法，但是很明显会很慢。这将会引出我们的下一个话题：Log compaction和Snapshot。

##### 日志快照（Log Snapshot）
Log压缩和快照（Log compaction and snapshots）在Lab3b中出现的较多。在Raft中，Log压缩和快照解决的问题是：对于一个长期运行的系统，例如运行了几周，几个月甚至几年，如果我们按照Raft论文图2的规则，那么Log会持续增长。最后可能会有数百万条Log，从而需要大量的内存来存储。如果持久化存储在磁盘上，最终会消耗磁盘的大量空间。如果一个服务器重启了，它需要通过重新从头开始执行这数百万条Log来重建自己的状态。当故障重启之后，遍历并执行整个Log的内容可能要花费几个小时来完成。这在某种程度上来说是浪费，因为在重启之前，服务器已经有了一定的应用程序状态。为了应对这种场景，Raft有了快照（Snapshots）的概念。快照背后的思想是，要求应用程序将其状态的拷贝作为一种特殊的Log条目存储下来。我们之前几乎都忽略了应用程序，但是事实是，假设我们基于Raft构建一个key-value数据库，Log将会包含一系列的Put/Get或者Read/Write请求。假设一条Log包含了一个Put请求，客户端想要将X设置成1，另一条Log想要将X设置成2，下一条将Y设置成7。如果Raft一直执行没有故障，Raft之上的将会是应用程序，在这里，应用程序将会是key-value数据库。它将会维护一个表单，当Raft一个接一个的上传命令时，应用程序会更新它的表单。所以第一个命令之后，应用程序会将表单中的X设置为1。第二个命令之后，表单中的X会被设置为2。第三个命令之后，表单中的Y会被设置为7。这里有个有趣的事实，那就是：对于大多数的应用程序来说，应用程序的状态远小于Log的大小。某种程度上我们知道，在某些时间点，Log和应用程序的状态是可以互换的，它们是用来表示应用程序状态的不同事物。但是Log可能包含大量的重复的记录（例如对于X的重复赋值），这些记录使用了Log中的大量的空间，但是同时却压缩到了key-value表单中的一条记录。这在多副本系统中很常见。在这里，如果存储Log，可能尺寸会非常大，相应的，如果存储key-value表单，这可能比Log尺寸小得多。这就是快照的背后原理。
所以，当Raft认为它的Log将会过于庞大，例如大于1MB，10MB或者任意的限制，Raft会要求应用程序在Log的特定位置，对其状态做一个快照。所以，如果Raft要求应用程序做一个快照，Raft会从Log中选取一个与快照对应的点，然后要求应用程序在那个点的位置做一个快照。这里极其重要，因为我们接下来将会丢弃所有那个点之前的Log记录。如果我们有一个点的快照，那么我们可以安全的将那个点之前的Log丢弃。（在key-value数据库的例子中）快照本质上就是key-value表单。我们还需要为快照标注Log的槽位号。在这个图里面，这个快照对应的正好是槽位3。有了快照，并且Raft将它存放在磁盘中之后，Raft将不会再需要这部分Log。只要Raft持久化存储了快照，快照对应的Log槽位号，以及Log槽位号之后的所有Log，那么快照对应槽位号之前的这部分Log可以被丢弃，我们将不再需要这部分Log。所以这就是Raft快照的工作原理，Raft要求应用程序做快照，得到快照之后将其存储在磁盘中，同时持久化存储快照之后的Log，并丢弃快照之前的Log。所以，Raft的持久化存储实际上是持久化应用程序快照，和快照之后的Log。大家都明白了吗？
刚刚的回答可能有些草率。因为如果按照Raft论文的图2，你有时还是需要这些早期的Log（槽位1，2，3）。所以，在知道了有时候某些Log可能不存在的事实之后，你可能需要稍微重新理解一下图2。
所以，重启的时候会发生什么呢？现在，重启的场景比之前只有Log会更加复杂一点。重启的时候，必须让Raft有方法知道磁盘中最近的快照和Log的组合，并将快照传递给应用程序。因为现在我们不能重演所有的Log（部分被删掉了），所以必须要有一种方式来初始化应用程序。所以应用程序不仅需要有能力能生成一个快照，它还需要能够吸纳一个之前创建的快照，并通过它稳定的重建自己的内存。所以，尽管Raft在管理快照，快照的内容实际上是应用程序的属性。Raft并不理解快照中有什么，只有应用程序知道，因为快照里面都是应用程序相关的信息。所以重启之后，应用程序需要能够吸纳Raft能找到的最近的一次快照。到目前为止还算简单。
不幸的是，这里丢弃了快照之前的Log，引入了大量的复杂性。如果有的Follower的Log较短，在Leader的快照之前就结束，那么除非有一种新的机制，否则那个Follower永远也不可能恢复完整的Log。因为，如果一个Follower只有前两个槽位的Log，Leader不再有槽位3的Log可以通过AppendEntries RPC发给Follower，Follower的Log也就不可能补齐至Leader的Log。

Raft选择的方法是，Leader可以丢弃Follower需要的Log。所以，我们需要某种机制让AppendEntries能处理某些Follower Log的结尾到Leader Log开始之间丢失的这一段Log。解决方法是（一个新的消息类型）InstallSnapshot RPC。当Follower刚刚恢复，如果它的Log短于Leader通过 AppendEntries RPC发给它的内容，那么它首先会强制Leader回退自己的Log。在某个点，Leader将不能再回退，因为它已经到了自己Log的起点。这时，Leader会将自己的快照发给Follower，之后立即通过AppendEntries将后面的Log发给Follower。

##### 线性一致（Linearizability）
接下来我们看一些更偏概念性的东西。目前为止，我们还没有尝试去确定正确意味着什么？当一个多副本服务或者任意其他服务正确运行意味着什么？ 绝大多数时候，我都避免去考虑太多有关正确的精确定义。但事实是，当你尝试去优化一些东西，或者当你尝试去想明白一些奇怪的corner case，如果有个正式的方式定义什么是正确的行为，经常会比较方便。例如，当客户端通过RPC发送请求给我们的多副本服务时，可能是请求重发，可能是服务故障重启正在加载快照，或者客户端发送了请求并且得到了返回，但是这个返回是正确的吗？我们该如何区分哪个返回是正确的？所以，我们需要一个非常正式的定义来区分，什么是对的，什么是错的。
我们对于正确的定义就是线性一致（Linearizability）或者说强一致（Strong consistency）。通常来说，线性一致等价于强一致。一个服务是线性一致的，那么它表现的就像只有一个服务器，并且服务器没有故障，这个服务器每次执行一个客户端请求，并且没什么奇怪的是事情发生。
一个系统的执行历史是一系列的客户端请求，或许这是来自多个客户端的多个请求。如果执行历史整体可以按照一个顺序排列，且排列顺序与客户端请求的实际时间相符合，那么它是线性一致的。当一个客户端发出一个请求，得到一个响应，之后另一个客户端发出了一个请求，也得到了响应，那么这两个请求之间是有顺序的，因为一个在另一个完成之后才开始。一个线性一致的执行历史中的操作是非并发的，也就是时间上不重合的客户端请求与实际执行时间匹配。并且，每一个读操作都看到的是最近一次写入的值。
首先，执行历史是对于客户端请求的记录，你可以从系统的输入输出理解这个概念，而不用关心内部是如何实现的。如果一个系统正在工作，我们可以通过输入输出的消息来判断，系统的执行顺序是不是线性一致的。接下来，我们通过两个例子来看，什么是线性一致的，什么不是。
线性一致这个概念里面的操作，是从一个点开始，到另一个点结束。所以，这里前一个点对应了客户端发送请求，后一个点对应了收到回复的时间。我们假设，在某个特定的时间，客户端发送了请求，将X设置为1。过了一会，在第二条竖线处，客户端收到了一个回复。客户端在第一条竖线发送请求，在第二条竖线收到回复。过了一会，这个客户端或者其他客户端再发送一个请求，要将X设置为2，并收到了相应的回复。同时，某个客户端发送了一个读X的请求，得到了2。在第一条竖线发送读请求，在这个点，也就是第二条竖线，收到了值是2的响应。同时，还有一个读X的请求，得到值是1的响应。
如果我们观察到了这样的输入输出（执行历史），那么这样的执行历史是线性一致的吗？生成这样结果的系统，是一个线性一致的系统吗？或者系统在这种场景下，可以生成线性一致的执行历史吗？如果执行历史不是线性一致的，那么至少在Lab3，我们会有一些问题。所以，我们要分析并弄清楚，这里是不是线性一致的？
要达到线性一致，我们需要为这里的4个操作生成一个线性一致的顺序。所以我们现在要确定顺序，对于这个顺序，有两个限制条件：
- 如果一个操作在另一个操作开始前就结束了，那么这个操作必须在执行历史中出现在另一个操作前面。
- 执行历史中，读操作，必须在相应的key的写操作之后。

所以，这里我们要为4个操作创建一个顺序，两个读操作，两个写操作。我会通过箭头来标识刚刚两个限制条件，这样生成出来的顺序就能满足前面的限制条件。第一个写结束之后，第二个写才开始。所以一个限制条件是，在总的顺序中，第一个写操作必须在第二个写操作前面。第一个读操作看到的是值2，那么在总的顺序中，这个读必然在第二个写操作后面，同时第二个写必须是离第一个读操作最近一次写。所以，这意味着，在总的顺序中，我们必须先看到对X写2，之后执行读X才能得到2。第二个读X得到的是值1。我们假设X的值最开始不是1，那么会有下图的关系，因为读必须在写之后。第二个读操作必须在第二个写操作之前执行，这样写X为1的操作才能成为第二个读操作最近一次写操作。或许还有一些其他的限制，但是不管怎样，我们将这些箭头展平成一个线性一致顺序来看看真实的执行历史，我们可以发现总的执行历史是线性一致的。首先是将X写1，之后是读X得到1，之后将X写2，之后读X得到2。（这里可以这么理解，左边是一个多副本系统的输入输出，因为分布式程序或者程序的执行，产生了这样的时序，但是在一个线性一致的系统中，实际是按照右边的顺序执行的操作。左边是实际时钟，右边是逻辑时钟。）

#### Zookeeper
- 第一个问题：对于通用的服务，API应该怎样设计，它可以被认为是一个通用的协调服务。
- 作为一个多副本系统，Zookeeper是一个容错的，通用的协调服务，它与其他系统一样，通过多副本来完成容错。
现在的问题是，当我们增加更多的服务器，我们在这里可以有4个，5个，或者7个服务器，系统会随着我们我们增加更多的CPU，更多的算力，而变得更快吗？假设每一个副本都运行在独立的电脑上，这样你会有更多的CPU，那么当副本变多时，你的实验代码会变得更快吗？
是的，并没有这回事说，当你加入更多的服务器时，服务就会变得更快。这绝对是正确的，当我们加入更多的服务器时，Leader几乎可以确定是一个瓶颈，因为Leader需要处理每一个请求，它需要将每个请求的拷贝发送给每一个其他服务器。当你添加更多的服务器时，你只是为现在的瓶颈（Leader节点）添加了更多的工作负载。所以是的，你并不能通过添加服务器来达到提升性能的目的，因为新增的服务器并没有实际完成任何工作，它们只是愉快的完成Leader交代的工作，它们并没有减少Leader的工作。每一个操作都经过Leader。所以，在这里，随着服务器数量的增加，性能反而会降低，因为Leader需要做的工作更多了。所以，在这个系统中，我们现在有这个问题：更多的服务器使得系统更慢了。所以这里有点让人失望，服务器的硬件并不能帮助提升性能。

或许最简单的可以用来利用这些服务器的方法，就是构建一个系统，让所有的写请求通过Leader下发。在现实世界中，大量的负载是读请求，也就是说，读请求（比写请求）多得多。比如，web页面，全是通过读请求来生成web页面，并且通常来说，写请求就相对少的多，对于很多系统都是这样的。所以，或许我们可以将写请求发给Leader，但是将读请求发给某一个副本，随便任意一个副本。
如果你有一个读请求，例如Lab3中的get请求，把它发给某一个副本而不是Leader。如果我们这么做了，对于写请求没有什么帮助，是我们将大量的读请求的负担从Leader移走了。现在对于读请求来说，有了很大的提升，因为现在，添加越多的服务器，我们可以支持越多的客户端读请求，因为我们将客户端的读请求分担到了不同的副本上。所以，现在的问题是，如果我们直接将客户端的请求发送给副本，我们能得到预期的结果吗？
是的，实时性是这里需要考虑的问题。Zookeeper作为一个类似于Raft的系统，如果客户端将请求发送给一个随机的副本，那个副本中肯定有一份Log的拷贝，这个拷贝随着Leader的执行而变化。假设在Lab3中，这个副本有一个key-value表，当它收到一个读X的请求，在key-value表中会有X的某个数据，这个副本可以用这个数据返回给客户端。所以，功能上来说，副本拥有可以响应来自客户端读请求的所有数据。这里的问题是，没有理由可以相信，除了Leader以外的任何一个副本的数据是最新（up to date）的。
这里有很多原因导致副本没有最新的数据，其中一个原因是，这个副本可能不在Leader所在的过半服务器中。对于Raft来说，Leader只会等待它所在的过半服务器中的其他follower对于Leader发送的AppendEntries消息的返回，之后Leader才会commit消息，并进行下一个操作。所以，如果这个副本不在过半服务器中，它或许永远也看不到写请求。又或许网络丢包了，这个副本永远没有收到这个写请求。所以，有可能Leader和过半服务器可以看见前三个请求，但是这个副本只能看见前两个请求，而错过了请求C。所以从这个副本读数据可能读到一个旧的数据。
即使这个副本看到了相应的Log条目，它可能收不到commit消息。Zookeeper的Zab与Raft非常相似，它先发出Log条目，之后，当Leader收到了过半服务器的回复，Leader会发送commit消息。然后这个副本可能没有收到这个commit消息。最坏的情况是，我之前已经说过，这个副本可能与Leader不在一个网络分区，或者与Leader完全没有通信，作为follower，完全没有方法知道它与Leader已经失联了，并且不能收到任何消息了（心跳呢？）。
所以，如果这里不做任何改变，并且我们想构建一个线性一致的系统，尽管在性能上很有吸引力，我们不能将读请求发送给副本，并且你也不应该在Lab3这么做，因为Lab3也应该是线性一致的。这里是线性一致阻止了我们使用副本来服务客户端。
对的，实际上，Zookeeper并不要求返回最新的写入数据。Zookeeper的方式是，放弃线性一致性。它对于这里问题的解决方法是，不提供线性一致的读。所以，因此，Zookeeper也不用为读请求提供最新的数据。它有自己有关一致性的定义，而这个定义不是线性一致的，因此允许为读请求返回旧的数据。所以，Zookeeper这里声明，自己最开始就不支持线性一致性，来解决这里的技术问题。如果不提供这个能力，那么（为读请求返回旧数据）就不是一个bug。这实际上是一种经典的解决性能和强一致之间矛盾的方法，也就是不提供强一致。

##### 一致保证（Consistency Guarantees）
Zookeeper的确有一些一致性的保证，用来帮助那些使用基于Zookeeper开发应用程序的人，来理解他们的应用程序，以及理解当他们运行程序时，会发生什么。与线性一致一样，这些保证与序列有关。Zookeeper有两个主要的保证：
- 第一个是写请求是线性一致的
- 第二个是任何一个客户端的请求，都会按照客户端指定的顺序来执行

这里的意思是，如果一个特定的客户端发送了一个写请求之后是一个读请求或者任意请求，那么首先，所有的写请求会以这个客户端发送的相对顺序，加入到所有客户端的写请求中（满足保证1）。所以，如果一个客户端说，先完成这个写操作，再完成另一个写操作，之后是第三个写操作，那么在最终整体的写请求的序列中，可以看到这个客户端的写请求以相同顺序出现（虽然可能不是相邻的）。所以，对于写请求，最终会以客户端确定的顺序执行。

##### 同步操作（sync）
Zookeeper有一个操作类型是sync，它本质上就是一个写请求。假设我知道你最近写了一些数据，并且我想读出你写入的数据，所以现在的场景是，我想读出Zookeeper中最新的数据。这个时候，我可以发送一个sync请求，它的效果相当于一个写请求，
所以它最终会出现在所有副本的Log中，尽管我只关心与我交互的副本，因为我需要从那个副本读出数据。接下来，在发送读请求时，我（客户端）告诉副本，在看到我上一次sync请求之前，不要返回我的读请求。
如果这里把sync看成是一个写请求，这里实际上符合了FIFO客户端请求序列，因为读请求必须至少要看到同一个客户端前一个写请求对应的状态。所以，如果我发送了一个sync请求之后，又发送了一个读请求。Zookeeper必须要向我返回至少是我发送的sync请求对应的状态。
不管怎么样，如果我需要读最新的数据，我需要发送一个sync请求，之后再发送读请求。这个读请求可以保证看到sync对应的状态，所以可以合理的认为是最新的。但是同时也要认识到，这是一个代价很高的操作，因为我们现在将一个廉价的读操作转换成了一个耗费Leader时间的sync操作。所以，如果不是必须的，那还是不要这么做。

##### Zookeeper API
- Zookeeper API实现了**mini-transaction** Zookeeper的特点：
- Zookeeper基于（类似于）Raft框架，所以我们可以认为它是，当然它的确是容错的，它在发生网络分区的时候，也能有正确的行为。
- 当我们在分析各种Zookeeper的应用时，我们也需要记住Zookeeper有一些性能增强，使得读请求可以在任何副本被处理，因此，可能会返回旧数据。
- 另一方面，Zookeeper可以确保一次只处理一个写请求，并且所有的副本都能看到一致的写请求顺序。这样，所有副本的状态才能保证是一致的（写请求会改变状态，一致的写请求顺序可以保证状态一致）。
- 由一个客户端发出的所有读写请求会按照客户端发出的顺序执行。
- 一个特定客户端的连续请求，后来的请求总是能看到相比较于前一个请求相同或者更晚的状态。

##### 使用Zookeeper实现可扩展锁
有意思的是，因为Zookeeper的API足够灵活，可以用来设计另一个更复杂的锁，从而避免羊群效应。从而使得，即使有1000个客户端在等待锁释放，当锁释放时，另一个客户端获得锁的复杂度是 而不是 。
```c
CREATE("f", data, sequential=TRUE, ephemeral=TRUE)
WHILE TRUE:
    LIST("f*")
    IF NO LOWER #FILE: RETURN
    IF EXIST(NEXT LOWER #FILE, watch=TRUE):
        WAIT
```
在代码的第1行调用CREATE，并指定sequential=TRUE，我们创建了一个Sequential文件，如果这是以“f”开头的第27个Sequential文件，这里实际会创建类似以“f27”为名字的文件。这里有两点需要注意，第一是通过CREATE，我们获得了一个全局唯一序列号（比如27），第二Zookeeper生成的序号必然是递增的。代码第3行，通过LIST列出了所有以“f”开头的文件，也就是所有的Sequential文件。代码第4行，如果现存的Sequential文件的序列号都不小于我们在代码第1行得到的序列号，那么表明我们在并发竞争中赢了，我们获得了锁。所以当我们的Sequential文件对应的序列号在所有序列号中最小时，我们获得了锁，直接RETURN。序列号代表了不同客户端创建Sequential文件的顺序。在这种锁方案中，会使用这个顺序来向客户端分发锁。当存在更低序列号的Sequential文件时，我们要做的是等待拥有更低序列号的客户端释放锁。在这个方案中，释放锁的方式是删除文件。所以接下来，我们需要做的是等待序列号更低的锁文件删除，之后我们才能获得锁。所以，在代码的第5行，我们调用EXIST，并设置WATCH，等待比自己序列号更小的下一个锁文件删除。如果等到了，我们回到循环的最开始。但是这次，我们不会再创建锁文件，代码从LIST开始执行。这是获得锁的过程，释放就是删除创建的锁文件。
假设我们有1000个客户端在等待获取锁，每个客户端都会在代码的第6行等待锁释放。但是每个客户端等待的锁文件都不一样，比如序列号为500的锁只会被序列号为501的客户端等待，而序列号500的客户端只会等待序列号499的锁文件。每个客户端只会等待一个锁文件，当一个锁文件被释放，只有下一个序列号对应的客户端才会收到通知，也只有这一个客户端会回到循环的开始，也就是代码的第3行，之后这个客户端会获得锁。所以，不管有多少个客户端在等待锁，每一次锁释放再被其他客户端获取的代价是一个常数。而在非扩展锁中，锁释放时，每个等待的客户端都会被通知到，之后，每个等待的客户端都会发送CREATE请求给Zookeeper，所以每一次锁释放再被其他客户端获取的代价与客户端数量成正比。

不得不说，我有点迷惑为什么Zookeeper论文要讨论锁。因为这里的锁并不像线程中的锁，在线程系统中，不存在线程随机的挂了然后下线。如果每个线程都正确使用了锁，你从线程锁中可以获得操作的原子性（Atomicity）。假如你获得了锁，并且执行了47个不同的读写操作，修改了一些变量，然后释放了锁。如果所有的线程都遵从这里的锁策略，没有人会看到一切奇怪的数据中间状态。这里的线程锁可以使得操作具备原子性。
而通过Zookeeper实现的锁就不太一样。如果持有锁的客户端挂了，它会释放锁，另一个客户端可以接着获得锁，所以它并不确保原子性。因为你在分布式系统中可能会有部分故障（Partial Failure），但是你在一个多线程代码中不会有部分故障。如果当前锁的持有者需要在锁释放前更新一系列被锁保护的数据，但是更新了一半就崩溃了，之后锁会被释放。然后你可以获得锁，然而当你查看数据的时候，只能看到垃圾数据，因为这些数据是只更新了一半的随机数据。所以，Zookeeper实现的锁，并没有提供类似于线程锁的原子性保证。

有两点需要注意：第一是Zookeeper聪明的从多个副本读数据从而提升了性能，但同时又牺牲了一些一致性；另一个是Zookeeper的API设计，使得Zookeeper成为一个通用的协调服务，这是一个简单的put/get 服务所不能实现，这些API使你可以写出类似mini-transaction的代码，也可以帮你创建自己的锁。

#### 链复制（Chain Replication）
第一个是它通过复制实现了容错；第二是它通过以链复制API请求这种有趣的方式，提供了与Raft相比不一样的属性。
CRAQ是对于一个叫链式复制（Chain Replication）的旧方案的改进。Chain Replication实际上用的还挺多的，有许多现实世界的系统使用了它，CRAQ是对它的改进。CRAQ采用的方式与Zookeeper非常相似，它通过将读请求分发到任意副本去执行，来提升读请求的吞吐量，所以副本的数量与读请求性能成正比。CRAQ有意思的地方在于，它在任意副本上执行读请求的前提下，还可以保证线性一致性（Linearizability）。这与Zookeeper不太一样，Zookeeper为了能够从任意副本执行读请求，不得不牺牲数据的实时性，因此也就不是线性一致的。CRAQ却可以从任意副本执行读请求，同时也保留线性一致性，这一点非常有趣。
首先，我想讨论旧的Chain Replication系统。Chain Replication是这样一种方案，你有多个副本，你想确保它们都看到相同顺序的写请求（这样副本的状态才能保持一致），这与Raft的思想是一致的，但是它却采用了与Raft不同的拓扑结构。首先，在Chain Replication中，有一些服务器按照链排列。第一个服务器称为HEAD，最后一个被称为TAIL。HEAD根据写请求更新本地数据，我们假设现在是一个支持PUT/GET的key-value数据库。所有的服务器本地数据都从A开始。HEAD根据写请求更新本地数据，我们假设现在是一个支持PUT/GET的key-value数据库。所有的服务器本地数据都从A开始。当HEAD收到了写请求，将本地数据更新成了B，之后会再将写请求通过链向下一个服务器传递。下一个服务器执行完写请求之后，再将写请求向下一个服务器传递，以此类推，所有的服务器都可以看到写请求。当写请求到达TAIL时，TAIL将回复发送给客户端，表明写请求已经完成了。这是处理写请求的过程。对于读请求，如果一个客户端想要读数据，它将读请求发往TAIL，TAIL直接根据自己的当前状态来回复读请求。所以，如果当前状态是B，那么TAIL直接返回B。读请求处理的非常的简单。这里只是Chain Replication，并不是CRAQ。Chain Replication本身是线性一致的，在没有故障时，从一致性的角度来说，整个系统就像只有TAIL一台服务器一样，TAIL可以看到所有的写请求，也可以看到所有的读请求，它一次只处理一个请求，读请求可以看到最新写入的数据。如果没有出现故障的话，一致性是这么得到保证的，非常的简单。


#### Auror数据库
为了向用户提供EC2实例所需的硬盘，并且硬盘数据不会随着服务器故障而丢失，就出现了一个与Aurora相关的服务，并且同时也是容错的且支持持久化存储的服务，这个服务就是EBS。EBS全称是Elastic Block Store。从EC2实例来看，EBS就是一个硬盘，你可以像一个普通的硬盘一样去格式化它，就像一个类似于ext3格式的文件系统或者任何其他你喜欢的Linux文件系统。但是在实现上，EBS底层是一对互为副本的存储服务器。随着EBS的推出，你可以租用一个EBS volume。一个EBS volume看起来就像是一个普通的硬盘一样，但却是由一对互为副本EBS服务器实现，每个EBS服务器本地有一个硬盘。所以，现在你运行了一个数据库，相应的EC2实例将一个EBS volume挂载成自己的硬盘。当数据库执行写磁盘操作时，数据会通过网络送到EBS服务器。
这两个EBS服务器会使用Chain Replication（9.5）进行复制。所以写请求首先会写到第一个EBS服务器，之后写到第二个EBS服务器，然后从第二个EBS服务器，EC2实例可以得到回复。当读数据的时候，因为这是一个Chain Replication，EC2实例会从第二个EBS服务器读取数据。
所以现在，运行在EC2实例上的数据库有了可用性。因为现在有了一个存储系统可以在服务器宕机之后，仍然能持有数据。如果数据库所在的服务器挂了，你可以启动另一个EC2实例，并为其挂载同一个EBS volume，再启动数据库。新的数据库可以看到所有前一个数据库留下来的数据，就像你把硬盘从一个机器拔下来，再插入到另一个机器一样。所以EBS非常适合需要长期保存数据的场景，比如说数据库。
对于我们来说，有关EBS有一件很重要的事情：这不是用来共享的服务。任何时候，只有一个EC2实例，一个虚机可以挂载一个EBS volume。所以，尽管所有人的EBS volume都存储在一个大的服务器池子里，每个EBS volume只能被一个EC2实例所使用。

- 第一个是，在替代EBS的位置，有6个数据的副本，位于3个AZ，每个AZ有2个副本。所以现在有了超级容错性，并且每个写请求都需要以某种方式发送给这6个副本。现在有了更多的副本，我的天，为什么Aurora不是更慢了，之前Mirrored MySQL中才有4个副本。答案是，这里通过网络传递的数据只有Log条目，这才是Aurora成功的关键。从之前的简单数据库模型可以看出，每一条Log条目只有几十个字节那么多，也就是存一下旧的数值，新的数值，所以Log条目非常小。然而，当一个数据库要写本地磁盘时，它更新的是data page，这里的数据是巨大的，虽然在论文里没有说，但是我认为至少是8k字节那么多。所以，对于每一次事务，需要通过网络发送多个8k字节的page数据。而Aurora只是向更多的副本发送了少量的Log条目。因为Log条目的大小比8K字节小得多，所以在网络性能上这里就胜出了。这是Aurora的第一个特点，只发送Log条目。当然，这里的后果是，这里的存储系统不再是通用（General-Purpose）存储，这是一个可以理解MySQL Log条目的存储系统。EBS是一个非常通用的存储系统，它模拟了磁盘，只需要支持读写数据块。EBS不理解除了数据块以外的其他任何事物。而这里的存储系统理解使用它的数据库的Log。所以这里，Aurora将通用的存储去掉了，取而代之的是一个应用定制的（Application-Specific）存储系统。

- 另一件重要的事情是，Aurora并不需要6个副本都确认了写入才能继续执行操作。相应的，只要Quorum形成了，也就是任意4个副本确认写入了，数据库就可以继续执行操作。所以，当我们想要执行写入操作时，如果有一个AZ下线了，或者AZ的网络连接太慢了，或者只是服务器响应太慢了，Aurora可以忽略最慢的两个服务器，或者已经挂掉的两个服务器，它只需要6个服务器中的任意4个确认写入，就可以继续执行。所以这里的Quorum是Aurora使用的另一个聪明的方法。通过这种方法，Aurora可以有更多的副本，更多的AZ，但是又不用付出大的性能代价，因为它永远也不用等待所有的副本，只需要等待6个服务器中最快的4个服务器即可。

Mirrored MySQL将大的page数据发送给4个副本，而Aurora只是将小的Log条目发送给6个副本，Aurora获得了35倍的性能提升。论文并没有介绍性能的提升中，有多少是Quorum的功劳，有多少是只发送Log条目的功劳，但是不管怎么样，35倍的性能提升是令人尊敬的结果，同时也是对用户来说非常有价值的结果。我相信对于许多Amazon的客户来说，这是具有革新意义的。

##### Aurora存储服务器的容错目标（Fault-Tolerant Goals）
- 首先是对于写操作，当只有一个AZ彻底挂了之后，写操作不受影响。
- 其次是对于读操作，当一个AZ和一个其他AZ的服务器挂了之后，读操作不受影响。这里的原因是，AZ的下线时间可能很长，比如说数据中心被水淹了。人们可能需要几天甚至几周的时间来修复洪水造成的故障，在AZ下线的这段时间，我们只能依赖其他AZ的服务器。如果其他AZ中的一个服务器挂了，我们不想让整个系统都瘫痪。所以当一个AZ彻底下线了之后，对于读操作，Aurora还能容忍一个额外服务器的故障，并且仍然可以返回正确的数据。至于为什么会定这样的目标，我们必须理所当然的认为Amazon知道他们自己的业务，并且认为这是实现容错的最佳目标。
- 此外，我之前也提过，Aurora期望能够容忍暂时的慢副本。如果你向EBS读写数据，你并不能得到稳定的性能，有时可能会有一些卡顿，或许网络中一部分已经过载了，或许某些服务器在执行软件升级，任何类似的原因会导致暂时的慢副本。所以Aurora期望能够在出现短暂的慢副本时，仍然能够继续执行操作。
- 最后一个需求是，如果一个副本挂了，在另一个副本挂之前，是争分夺秒的。统计数据或许没有你期望的那么好，因为通常来说服务器故障不是独立的。事实上，一个服务器挂了，通常意味着有很大的可能另一个服务器也会挂，因为它们有相同的硬件，或许从同一个公司购买，来自于同一个生产线。如果其中一个有缺陷，非常有可能会在另一个服务器中也会有相同的缺陷。所以，当出现一个故障时，人们总是非常紧张，因为第二个故障可能很快就会发生。对于Aurora的Quorum系统，有点类似于Raft，你只能从局部故障中恢复。所以这里需要快速生成新的副本（Fast Re-replication）。也就是说如果一个服务器看起来永久故障了，我们期望能够尽可能快的根据剩下的副本，生成一个新的副本。

##### Quorum 复制机制（Quorum Replication）
相比Chain Replication，这里的优势是可以轻易的剔除暂时故障、失联或者慢的服务器。实际上，这里是这样工作的，当你执行写请求时，你会将新的数值和对应的版本号给所有N个服务器，但是只会等待W个服务器确认。类似的，对于读请求，你可以将读请求发送给所有的服务器，但是只等待R个服务器返回结果。因为你只需要等待R个服务器，这意味着在最快的R个服务器返回了之后，你就可以不用再等待慢服务器或者故障服务器超时。这里忽略慢服务器或者挂了的服务器的机制完全是隐式的。在这里，我们不用决定哪个服务器是在线或者是离线的，只要Quorum能达到，系统就能继续工作，所以我们可以非常平滑的处理慢服务或者挂了的服务。
除此之外，Quorum系统可以调整读写的性能。通过调整Read Quorum和Write Quorum，可以使得系统更好的支持读请求或者写请求。对于前面的例子，我们可以假设Write Quorum是3，每一个写请求必须被所有的3个服务器所确认。这样的话，Read Quorum可以只是1。所以，如果你想要提升读请求的性能，在一个3个服务器的Quorum系统中，你可以设置R为1，W为3，这样读请求会快得多，因为它只需要等待一个服务器的结果，但是代价是写请求执行的比较慢。如果你想要提升写请求的性能，可以设置R为3，W为1，这意味着可能只有1个服务器有最新的数值，但是因为客户端会咨询3个服务器，3个服务器其中一个肯定包含了最新的数值。
当R为1，W为3时，写请求就不再是容错的了，同样，当R为3，W为1时，读请求不再是容错的，因为对于读请求，所有的服务器都必须在线才能执行成功。所以在实际场景中，你不会想要这么配置，你或许会与Aurora一样，使用更多的服务器，将N变大，然后再权衡Read Quorum和Write Quorum。
为了实现上一节描述的Aurora的容错目标，也就是在一个AZ完全下线时仍然能写，在一个AZ加一个其他AZ的服务器下线时仍然能读，Aurora的Quorum系统中，N=6，W=4，R=3。W等于4意味着，当一个AZ彻底下线时，剩下2个AZ中的4个服务器仍然能完成写请求。R等于3意味着，当一个AZ和一个其他AZ的服务器下线时，剩下的3个服务器仍然可以完成读请求。当3个服务器下线了，系统仍然支持读请求，仍然可以返回当前的状态，但是却不能支持写请求。所以，当3个服务器挂了，现在的Quorum系统有足够的服务器支持读请求，并据此重建更多的副本，但是在新的副本创建出来替代旧的副本之前，系统不能支持写请求。同时，如我之前解释的，Quorum系统可以剔除暂时的慢副本。

##### Aurora读写存储服务器
Aurora中的写请求并不是像一个经典的Quorum系统一样直接更新数据。对于Aurora来说，它的写请求从来不会覆盖任何数据，它的写请求只会在当前Log中追加条目（Append Entries）。所以，Aurora使用Quorum只是在数据库执行事务并发出新的Log记录时，确保Log记录至少出现在4个存储服务器上，之后才能提交事务。所以，Aurora的Write Quorum的实际意义是，每个新的Log记录必须至少追加在4个存储服务器中，之后才可以认为写请求完成了。当Aurora执行到事务的结束，并且在回复给客户端说事务已经提交之前，Aurora必须等待Write Quorum的确认，也就是4个存储服务器的确认，组成事务的每一条Log都成功写入了。
实际上，在一个故障恢复过程中，事务只能在之前所有的事务恢复了之后才能被恢复。所以，实际中，在Aurora确认一个事务之前，它必须等待Write Quorum确认之前所有已提交的事务，之后再确认当前的事务，最后才能回复给客户端。这里的存储服务器接收Log条目，这是它们看到的写请求。它们并没有从数据库服务器获得到新的data page，它们得到的只是用来描述data page更新的Log条目。但是存储服务器内存最终存储的还是数据库服务器磁盘中的page。在存储服务器的内存中，会有自身磁盘中page的cache，例如page1（P1），page2（P2），这些page其实就是数据库服务器对应磁盘的page。

当一个新的写请求到达时，这个写请求只是一个Log条目，Log条目中的内容需要应用到相关的page中。但是我们不必立即执行这个更新，可以等到数据库服务器或者恢复软件想要查看那个page时才执行。对于每一个存储服务器存储的page，如果它最近被一个Log条目修改过，那么存储服务器会在内存中缓存一个旧版本的page和一系列来自于数据库服务器有关修改这个page的Log条目。所以，对于一个新的Log条目，它会立即被追加到影响到的page的Log列表中。这里的Log列表从上次page更新过之后开始（相当于page是snapshot，snapshot后面再有一系列记录更新的Log）。如果没有其他事情发生，那么存储服务器会缓存旧的page和对应的一系列Log条目。
如果之后数据库服务器将自身缓存的page删除了，过了一会又需要为一个新的事务读取这个page，它会发出一个读请求。请求发送到存储服务器，会要求存储服务器返回当前最新的page数据。在这个时候，存储服务器才会将Log条目中的新数据更新到page，并将page写入到自己的磁盘中，之后再将更新了的page返回给数据库服务器。同时，存储服务器在自身cache中会删除page对应的Log列表，并更新cache中的page，虽然实际上可能会复杂的多。
如刚刚提到的，数据库服务器有时需要读取page。所以，可能你已经发现了，数据库服务器写入的是Log条目，但是读取的是page。这也是与Quorum系统不一样的地方。Quorum系统通常读写的数据都是相同的。除此之外，在一个普通的操作中，数据库服务器可以避免触发Quorum Read。数据库服务器会记录每一个存储服务器接收了多少Log。所以，首先，Log条目都有类似12345这样的编号，当数据库服务器发送一条新的Log条目给所有的存储服务器，存储服务器接收到它们会返回说，我收到了第79号和之前所有的Log。数据库服务器会记录这里的数字，或者说记录每个存储服务器收到的最高连续的Log条目号。这样的话，当一个数据库服务器需要执行读操作，它只会挑选拥有最新Log的存储服务器，然后只向那个服务器发送读取page的请求。所以，数据库服务器执行了Quorum Write，但是却没有执行Quorum Read。因为它知道哪些存储服务器有最新的数据，然后可以直接从其中一个读取数据。这样的代价小得多，因为这里只读了一个副本，而不用读取Quorum数量的副本。
但是，数据库服务器有时也会使用Quorum Read。假设数据库服务器运行在某个EC2实例，如果相应的硬件故障了，数据库服务器也会随之崩溃。在Amazon的基础设施有一些监控系统可以检测到Aurora数据库服务器崩溃，之后Amazon会自动的启动一个EC2实例，在这个实例上启动数据库软件，并告诉新启动的数据库：你的数据存放在那6个存储服务器中，请清除存储在这些副本中的任何未完成的事务，之后再继续工作。这时，Aurora会使用Quorum的逻辑来执行读请求。因为之前数据库服务器故障的时候，它极有可能处于执行某些事务的中间过程。所以当它故障了，它的状态极有可能是它完成并提交了一些事务，并且相应的Log条目存放于Quorum系统。同时，它还在执行某些其他事务的过程中，这些事务也有一部分Log条目存放在Quorum系统中，但是因为数据库服务器在执行这些事务的过程中崩溃了，这些事务永远也不可能完成。对于这些未完成的事务，我们可能会有这样一种场景，第一个副本有第101个Log条目，第二个副本有第102个Log条目，第三个副本有第104个Log条目，但是没有一个副本持有第103个Log条目。
所以故障之后，新的数据库服务器需要恢复，它会执行Quorum Read，找到第一个缺失的Log序号，在上面的例子中是103，并说，好吧，我们现在缺失了一个Log条目，我们不能执行这条Log之后的所有Log，因为我们缺失了一个Log对应的更新。
所以，这种场景下，数据库服务器执行了Quorum Read，从可以连接到的存储服务器中发现103是第一个缺失的Log条目。这时，数据库服务器会给所有的存储服务器发送消息说：请丢弃103及之后的所有Log条目。103及之后的Log条目必然不会包含已提交的事务，因为我们知道只有当一个事务的所有Log条目存在于Write Quorum时，这个事务才会被commit，所以对于已经commit的事务我们肯定可以看到相应的Log。这里我们只会丢弃未commit事务对应的Log条目。
所以，某种程度上，我们将Log在102位置做了切割，102及之前的Log会保留。但是这些会保留的Log中，可能也包含了未commit事务的Log，数据库服务器需要识别这些Log。这是可行的，可以通过Log条目中的事务ID和事务的commit Log条目来判断（10.3）哪些Log属于已经commit的事务，哪些属于未commit的事务。数据库服务器可以发现这些未完成的事务对应Log，并发送undo操作来撤回所有未commit事务做出的变更。这就是为什么Aurora在Log中同时也会记录旧的数值的原因。因为只有这样，数据库服务器在故障恢复的过程中，才可以回退之前只提交了一部分，但是没commit的事务。

##### 数据分片（Protection Group）
为了能支持超过10TB数据的大型数据库。Amazon的做法是将数据库的数据，分割存储到多组存储服务器上，每一组都是6个副本，分割出来的每一份数据是10GB。所以，如果一个数据库需要20GB的数据，那么这个数据库会使用2个PG（Protection Group），其中一半的10GB数据在一个PG中，包含了6个存储服务器作为副本，另一半的10GB数据存储在另一个PG中，这个PG可能包含了不同的6个存储服务器作为副本。
因为Amazon运行了大量的存储服务器，这些服务器一起被所有的Aurora用户所使用。两组PG可能使用相同的6个存储服务器，但是通常来说是完全不同的两组存储服务器。随着数据库变大，我们可以有更多的Protection Group。这里有一件有意思的事情，你可以将磁盘中的data page分割到多个独立的PG中，比如说奇数号的page存在PG1，偶数号的page存在PG2。如果可以根据data page做sharding，那是极好的。
因为Amazon运行了大量的存储服务器，这些服务器一起被所有的Aurora用户所使用。两组PG可能使用相同的6个存储服务器，但是通常来说是完全不同的两组存储服务器。随着数据库变大，我们可以有更多的Protection Group。
这里有一件有意思的事情，你可以将磁盘中的data page分割到多个独立的PG中，比如说奇数号的page存在PG1，偶数号的page存在PG2。如果可以根据data page做sharding，那是极好的。
Sharding之后，Log该如何处理就不是那么直观了。如果有多个Protection Group，该如何分割Log呢？答案是，当Aurora需要发送一个Log条目时，它会查看Log所修改的数据，并找到存储了这个数据的Protection Group，并把Log条目只发送给这个Protection Group对应的6个存储服务器。这意味着，每个Protection Group只存储了部分data page和所有与这些data page关联的Log条目。所以每个Protection Group存储了所有data page的一个子集，以及这些data page相关的Log条目。
如果其中一个存储服务器挂了，我们期望尽可能快的用一个新的副本替代它。因为如果4个副本挂了，我们将不再拥有Read Quorum，我们也因此不能创建一个新的副本。所以我们想要在一个副本挂了以后，尽可能快的生成一个新的副本。表面上看，每个存储服务器存放了某个数据库的某个某个Protection Group对应的10GB数据，但实际上每个存储服务器可能有1-2块几TB的磁盘，上面存储了属于数百个Aurora实例的10GB数据块。所以在存储服务器上，可能总共会有10TB的数据，当它故障时，它带走的不仅是一个数据库的10GB数据，同时也带走了其他数百个数据库的10GB数据。所以生成的新副本，不是仅仅要恢复一个数据库的10GB数据，而是要恢复存储在原来服务器上的整个10TB的数据。我们来做一个算术，如果网卡是10Gb/S，通过网络传输10TB的数据需要8000秒。这个时间太长了，我们不想只是坐在那里等着传输。所以我们不想要有这样一种重建副本的策略：找到另一台存储服务器，通过网络拷贝上面所有的内容到新的副本中。我们需要的是一种快的多的策略。
Aurora实际使用的策略是，对于一个特定的存储服务器，它存储了许多Protection Group对应的10GB的数据块。对于Protection Group A，它的其他副本是5个服务器。